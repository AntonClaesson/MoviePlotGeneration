{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c7ffc5",
   "metadata": {},
   "source": [
    "### TODO: \n",
    "\n",
    "**Fix class imbalance** \n",
    "  * using NLP data augmentation https://neptune.ai/blog/data-augmentation-nlp So for examples containing rare genres and no common genres we can oversample with augmentation techniques. Problem: We have multi-label problem (many genres possible). To decide how to oversample we can have a \"repeat ceiling\". Better way would be to utlize \n",
    "  https://medium.com/thecyphy/handling-data-imbalance-in-multi-label-classification-mlsmote-531155416b87\n",
    "  https://link.springer.com/chapter/10.1007/978-3-642-40846-5_16 to decide which examples to oversample.\n",
    "  \n",
    "  \n",
    "**Fine-tune using distilgpt2** \n",
    "  * GPT2 is too large for our GPU so we use distilled version https://huggingface.co/distilgpt2\n",
    "  * For best performance: Make LM dataset using plots only and finetune the model a bit on this. (Large text document with each plot after the other. See example notebook from hugging face)\n",
    "  * Once the model outputs plot-like text, we want to train using the labeled dataset with genres and title.\n",
    "  \n",
    "** to start **\n",
    "Since we won't have time to do everything probably, let's start with the current dataset and just try to see what happens if we simply finetune w/o any augmentation and no pre-training on plot text.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd26678",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "This notebook implements a pre-trained GPT language model to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e454366d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets git-lfs ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a8f38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa284b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba8935",
   "metadata": {},
   "source": [
    "# START\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a559a305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    TrainerCallback,\n",
    "    GPT2Config,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    AdamW,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82b4830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_from_checkpoint = False\n",
    "\n",
    "# Load pretrained tokenizer and model\n",
    "finetuned_model_name = 'movie-plot-generator'\n",
    "\n",
    "if start_from_checkpoint:\n",
    "    config=AutoConfig.from_pretrained(finetuned_model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(finetuned_model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(finetuned_model_name, config=config)\n",
    "else:\n",
    "    model_name = 'distilgpt2' \n",
    "    config=AutoConfig.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, config=config)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e16326c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check of pre-trained model \n",
    "if start_from_checkpoint:\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    stories = generator(\"<BOS> <action> Shrek in the swamp <SEP> He was in the \", max_length=200, num_return_sequences=2)\n",
    "    print(*[story['generated_text'] + \"\\n\\n\\n------------------------\\n\" for story in stories])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d54c69",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "\n",
    "First, we load the dataset and split into train and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c379c5e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2fcf8d2135508f85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to C:\\Users\\Anton\\.cache\\huggingface\\datasets\\text\\default-2fcf8d2135508f85\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea539174f0c47dc95fab673eeb6e45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da94afb3ec5e4b80b03737640c0a16e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset text downloaded and prepared to C:\\Users\\Anton\\.cache\\huggingface\\datasets\\text\\default-2fcf8d2135508f85\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d933d35cf9a462db88310167aaf9c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092822f2530c4c4d9db9e146964fae95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=37.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558a9a396cc742549ed729be9c94e3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36475\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 556\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from text file called \"data.txt\" and split into train/val\n",
    "datasets = load_dataset(\"text\", data_files=\"data_top_15_genres.txt\")['train']\n",
    "datasets = datasets.train_test_split(train_size=0.985, seed=42)\n",
    "datasets['validation'] = datasets.pop('test')\n",
    "\n",
    "# Removal of unwanted text parts if necessary \n",
    "def processText(example):\n",
    "    #example['text'] = [ re.sub('<ref?(.*?)}}', '', text) for text in example['text'] ]\n",
    "    #example['text'] = [ re.sub('<ref name?(.*?)<EOS>', ' <EOS>', text) for text in example['text'] ]\n",
    "    #example['text'] = [ re.sub(' <!--', '', text) for text in example['text'] ]\n",
    "    return example\n",
    "\n",
    "datasets = datasets.map(processText, batched=True)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f79271",
   "metadata": {},
   "source": [
    "As can be seen, the examples are of different lengths. Examples longer than 1024 tokens needs to be truncated as this is the maximum input to GPT2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "490cccae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <action> Gundaraj <SEP> Ajay Chauhan lives with his parents and younger sister. He is in love with Pooja, and hopes to marry her someday. His father wants him to get a job and settle down, and then get married. Ajay applies for a job in Bombay, and soon receives a letter asking him to appear for an interview. He attends the interview, and is hired. Delighted to see all his dreams coming true, he goes to offer his thanks to God, and it is there a woman named Pratika Jetley sees him and notifies the police that he is indeed the one who had brutally raped three young women in a college campus. Ajay vehemently denies this, but is personally identified and criminally held responsible, convicted and sentenced to prison. Several years later he is released from prison, and finds out that his father and Pooja had committed suicide while his mother and sister are untraceable. He sets out to put his life together and meets with a ruthless police inspector, whose daughter was one of the rape victims. It is then Ajay finds out about the conspiracy behind this plot to frame him. <EOS>\n",
      "\n",
      "<BOS> <drama> Kon Khon <SEP> Chart, an oprhan, was raised by his master after his parent's death and trained be to be a Khon dancer, but he becomes involved with a dance teacher named Rambai. <EOS>\n",
      "\n",
      "<BOS> <drama> The Blue Max <SEP> German Corporal Bruno Stachel  leaves the fighting in the trenches to become an officer and fighter pilot in the German Army Air Service. Joining a squadron in spring 1918, he sets his sights on winning Imperial Germany's highest military decoration for valor, the Pour le MÃ©rite, nicknamed the \"Blue Max\", for which he must shoot down 20 aircraft. Of humble origins, Lieutenant Stachel is driven to prove himself better than the aristocratic pilots in his new fighter squadron, especially Willi von Klugermann . Their commanding officer, Hauptmann Otto Heidemann  is an upper-class officer whose notions of chivalry conflict with Stachel's ruthless determination. On his first mission, Stachel, in a Pfalz D.III, shoots down a British S.E.5, but does not receive credit for his \"kill\" because there were no witnesses. Stachel searches the French countryside for hours in a pouring rain for the wreckage, giving the impression that he cares more about scoring kills than the death of the man with whom he flew. Soon afterward, he attacks an Allied two-man observation aircraft, incapacitating the rear gunner. Instead of downing the defenseless aircraft, he signals the pilot to fly to the German base. As they near the airfield, the wounded rear gunner revives and reaches for his machine gun, unseen by the admiring observers on the ground. Stachel is forced to shoot the aircraft down, but Heidemann believes Stachel simply murdered a helpless enemy crew to gain a confirmed kill. The incident brings Stachel to the attention of General Count von Klugermann , Willi's uncle. When the general comes to the base to award his nephew the Blue Max, he meets Stachel. The general sees great propaganda potential in Stachel, one of the masses. Kaeti , the general's wife, is carrying on a discreet affair with her nephew by marriage. Soon afterward, Stachel is shot down after rescuing a red Fokker Dr.I attacked by two British fighters. When he returns to the airfield, he is stunned when he is introduced to the man he saved: Manfred von Richthofen , the Red Baron. Von Richthofen offers Stachel a place in his squadron, which Stachel declines, explaining his desire to \"prove himself\" with his current squadron. With Stachel temporarily grounded owing to a minor injury, General von Klugermann orders him to Berlin to help shore up crumbling public morale. Kaeti takes the opportunity to sleep with her latest hero. When Stachel returns to duty, he and Willi von Klugermann volunteer to escort a reconnaissance aircraft. British fighters attack their Fokker Dr. 1 triplanes. Stachel's guns jam, but Willi downs two of the enemy on his first pass, then a third on Stachel's tail, and the rest disengage. As the two are returning to their base, Willi challenges Stachel. Spotting a bridge, Willi dives under the wide middle span, but Stachel tops him by flying under a much narrower side one. Seething, Willi does the same, but clips the top of a tower afterward and crashes. When Stachel reports his death, Heidemann assumes that the two verified victories were Willi's. Insulted, Stachel impulsively claims them, even though it is discovered that he only fired 40 bullets before his guns jammed. Outraged, Heidemann reports Stachel to his superiors, but is told that Stachel's victories will be confirmed. Later, alone with Kaeti, Stachel admits he lied. During a strafing mission covering the retreat of the German army, Stachel disobeys Heidemann's order not to engage enemy fighters; one by one, the rest of the squadron follow him. Afterward, Heidemann has Stachel arrested, furious that nearly half the pilots were killed in the ensuing dogfight. Stachel cares only that he has shot down enough aircraft, even without Willi's kills, to qualify for the Blue Max. The two men are ordered to Berlin. There, von Klugermann tells Heidemann privately that Stachel is to receive the Blue Max, explaining that the people need a hero. Heidemann resigns his command when the general orders him to withdraw his report; he accepts a desk job. Later that evening, the countess visits Stachel and suggests that they run away to Switzerland since Germany's defeat is inevitable. She storms out when he refuses to give up flying. The next day, Stachel is awarded the Blue Max by the Crown Prince  in a well-publicized ceremony. However, a field marshal telephones von Klugermann to inform him of an impending investigation into Stachel's false claim. The general asks how the field marshal found out. While listening on the phone, he turns to his wife. When Heidemann reports that the new monoplane he has just test-flown is a \"death trap\", as its struts are dangerously weak, von Klugermann sees a way to avoid scandal. He orders Stachel to fly the aircraft and tells him, \"Let's see some real flying.\" The stress of Stachel's aerobatics causes the aircraft to crash. <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "print(datasets['train'][0]['text'] + '\\n')\n",
    "print(datasets['train'][1]['text'] + '\\n')\n",
    "print(datasets['train'][3]['text'] + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbb865",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "We now need to tokenize the dataset. The original tokenizer don't have all special tokens we require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "595d00ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(*tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d5c9e",
   "metadata": {},
   "source": [
    "We need to add the special tokens that we use in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ac15f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of added genres: 15\n",
      "50276 50276\n",
      "<BOS> <EOS> <|endoftext|> <SEP> <PAD> <romantic drama> <short film> <family film> <adventure> <action/adventure> <indie> <black-and-white> <horror> <crime fiction> <world cinema> <action> <thriller> <romance film> <comedy> <drama>\n"
     ]
    }
   ],
   "source": [
    "# Set special tokens\n",
    "tokenizer.bos_token = '<BOS>'\n",
    "tokenizer.eos_token = '<EOS>'\n",
    "tokenizer.pad_token = '<PAD>'\n",
    "tokenizer.sep_token = '<SEP>'\n",
    "\n",
    "# Add special tokens for each genre\n",
    "genres = ['romantic drama', 'short film', 'family film',\n",
    "          'adventure', 'action/adventure', 'indie',\n",
    "          'black-and-white', 'horror', 'crime fiction',\n",
    "          'world cinema', 'action', 'thriller', \n",
    "          'romance film', 'comedy', 'drama']\n",
    "\n",
    "print(f'Number of added genres: {len(genres)}')\n",
    "new_special_tokens = ['<BOS>', '<EOS>', '<PAD>', '<SEP>']\n",
    "new_special_tokens.extend([f'<{genre}>' for genre in genres])\n",
    "special_tokens = tokenizer.additional_special_tokens\n",
    "\n",
    "special_tokens.extend(new_special_tokens) \n",
    "new_special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "num_added_toks = tokenizer.add_special_tokens(new_special_tokens_dict)\n",
    "\n",
    "# We must resize token embeddings since new special tokens were added\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(model.config.vocab_size, tokenizer.vocab_size + len(tokenizer.get_added_vocab()))\n",
    "assert(model.config.vocab_size == tokenizer.vocab_size + len(tokenizer.get_added_vocab()))\n",
    "print(*tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "853e6b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS>', 'Ä ', '<drama>', 'Ä He', 'Ä was']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('<BOS> <drama> He was' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d6e30",
   "metadata": {},
   "source": [
    "**Tokenize the dataset**\n",
    "\n",
    "We tokenize the dataset. The tokenized examples contain the column names 'attention_mask' which is a mask for padding tokens and 'input_ids' which is the id of each token corrsponding to a word. We drop the text as that is not needed anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22681185",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e2f930cf4843dda17322def8bec94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=37.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155062cba7b34c13aa0005d55cd6d2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    padding='max_length' to pad to a length specified by the max_length argument \n",
    "    or the maximum length accepted by the model.\n",
    "    truncation=True to truncate each sequence to the maximum length accepted by the model\n",
    "    \"\"\"\n",
    "    result = tokenizer(examples[\"text\"], padding='max_length', truncation=True) # Max input according to model(1024)\n",
    "    #result = tokenizer(examples[\"text\"], max_length=512, padding='max_length', truncation=True)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f618dc0",
   "metadata": {},
   "source": [
    "Note that we duplicate the inputs to add our labels. This is because the model of the ðŸ¤— Transformers library apply the shifting to the right, so we don't need to do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8713cdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels'],\n",
       "        num_rows: 36475\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels'],\n",
       "        num_rows: 556\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make dataset format pytorch tensors\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbd91dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, extract the datasets and select a subset if wanted\n",
    "train_set = tokenized_datasets['train']#.select(list(range(10)))\n",
    "valid_set = tokenized_datasets['validation']#.select(list(range(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "002c34dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 36475\n",
      "}) Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 556\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_set, valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fefa121",
   "metadata": {},
   "source": [
    "### Training\n",
    "First, setup training args.\n",
    "The last argument to setup everything so we can push the model to the Hub regularly during training..\n",
    "\n",
    "Then pass training args to Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e6034ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveTokenizer(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback used to save the tokenizer whenever a model checkpoint is saved.\n",
    "    \"\"\"\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        tokenizer.save_pretrained(f\"./{finetuned_model_name}/\")\n",
    "\n",
    "        \n",
    "ce_loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    The compute function needs to receive a tuple (with logits and labels)\n",
    "    and has to return a dictionary with string keys (the name of the metric) and float values.\n",
    "    It will be called at the end of each evaluation phase on the whole arrays of predictions/labels.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    # Calculate perplexity https://huggingface.co/transformers/perplexity.html\n",
    "    # \"the exponentiation of the cross-entropy between the data and model predictions.\"\n",
    "    \n",
    "    perplexity = math.exp(ce_loss(logits, labels))\n",
    "    \n",
    "    return {'perplexity': perplexity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ce3eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "batch_size = 2 # 1:34:39 for one epoch (no evaluation steps) with batch_size = 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    finetuned_model_name,\n",
    "    evaluation_strategy = \"no\",\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-6,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=1,\n",
    "    save_steps=2500,\n",
    "    save_total_limit=1,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=valid_set,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[SaveTokenizer],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3df559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 36475\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 18238\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1891' max='18238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1891/18238 08:57 < 1:17:28, 3.52 it/s, Epoch 0.10/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>10.392100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.612000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results=trainer.train()\n",
    "pickle.dump(train_results, open(\"train_results.pickle\", \"wb\")) #Load: train_results = pickle.load(open(\"train_results.pickle\", \"rb\"))\n",
    "\n",
    "model.save_pretrained(f\"./{finetuned_model_name}/\")\n",
    "tokenizer.save_pretrained(f\"./{finetuned_model_name}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6719995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c608d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(finetuned_model_name)\n",
    "trainer.push_to_hub(finetuned_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89be0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference test\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "stories = generator(\"<BOS> <horror> Thovie <SEP>\", max_length=1024, num_return_sequences=4)\n",
    "print(*[story['generated_text'] + \"\\n\\n\\n------------------------\\n\" for story in stories])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d7364",
   "metadata": {},
   "source": [
    "### Push to HUB\n",
    "\n",
    "Push tokenizer and model to hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094df378",
   "metadata": {},
   "source": [
    "------\n",
    "------\n",
    "### Casual language modeling ## \n",
    "For causal language modeling (CLM) we are going to take all the texts in our dataset and concatenate them after they are tokenized. Then we will split them in examples of a certain sequence length. This way the model will receive chunks of contiguous text that may look like:\n",
    "    \n",
    "    part of text 1\n",
    "    \n",
    "or\n",
    "\n",
    "    end of text 1 <BOS_TOKEN> beginning of text 2\n",
    "    \n",
    " \n",
    "depending on whether they span over several of the original texts in the dataset or not.\n",
    "**Also the labels will be the same as the inputs, shifted to the left.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea116cb2",
   "metadata": {},
   "source": [
    "Now for the harder part: we need to concatenate all our texts together then split the result in small chunks of a certain block_size. To do this, we will use the map method again, with the option batched=True. This option actually lets us change the number of examples in the datasets by returning a different number of examples than we got. This way, we can create our new samples from a batch of examples.\n",
    "First, we grab the maximum length our model was pretrained with. This might be a big too big to fit in our GPU RAM, in that case decrease the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0410fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#block_size = tokenizer.model_max_length\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5821617",
   "metadata": {},
   "source": [
    "Then we write the preprocessing function that will group our texts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508479f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905bd446",
   "metadata": {},
   "source": [
    "First note that we duplicate the inputs for our labels. This is because the model of the ðŸ¤— Transformers library apply the shifting to the right, so we don't need to do it manually.\n",
    "\n",
    "Also note that by default, the map method will send a batch of 1,000 examples to be treated by the preprocessing function. So here, we will drop the remainder to make the concatenated tokenized texts a multiple of block_size every 1,000 examples. You can adjust this behavior by passing a higher batch size (which will also be processed slower). You can also speed-up the preprocessing by using multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb277ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")\n",
    "print(lm_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60be398d",
   "metadata": {},
   "source": [
    "And we can check our datasets have changed: now the samples contain chunks of block_size contiguous tokens, potentially spanning over several of our original texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e003957",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(lm_datasets[\"train\"][0][\"input_ids\"]))\n",
    "print()\n",
    "print(tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399c21f",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned, we're ready to instantiate our Trainer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
