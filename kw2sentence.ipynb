{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371c61c0",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "This notebook implements a pre-trained GPT language model to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c54df45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (4.11.3)\n",
      "Requirement already satisfied: Datasets in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (1.12.1)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from Datasets) (5.0.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from Datasets) (4.62.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from Datasets) (2.25.1)\n",
      "Requirement already satisfied: packaging in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from Datasets) (20.9)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from Datasets) (2021.10.0)\n",
      "Requirement already satisfied: xxhash in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from Datasets) (2.0.2)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from Datasets) (0.0.19)\n",
      "Requirement already satisfied: pandas in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from Datasets) (1.2.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from Datasets) (1.20.1)\n",
      "Requirement already satisfied: aiohttp in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from Datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: dill in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from Datasets) (0.3.4)\n",
      "Requirement already satisfied: multiprocess in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from Datasets) (0.70.12.2)\n",
      "Requirement already satisfied: pyyaml in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.14->Datasets) (5.4.1)\n",
      "Requirement already satisfied: filelock in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.14->Datasets) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.14->Datasets) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from packaging->Datasets) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->Datasets) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->Datasets) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->Datasets) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->Datasets) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->Datasets) (5.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->Datasets) (1.7.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->Datasets) (20.3.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->Datasets) (3.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from pandas->Datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from pandas->Datasets) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->Datasets) (1.15.0)\n",
      "Requirement already satisfied: click in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd35e7",
   "metadata": {},
   "source": [
    "# START\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11eb35d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "from transformers import pipeline\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TextDataset,\n",
    "    set_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aab1ff50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50266, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50266, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained tokenizer and model\n",
    "model_name = \"pranavpsv/gpt2-genre-story-generator\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c9fa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <superhero> Shrek and Aunt May look after their two young nephews, and their son, after they are kicked out of their house, and Uncle Vernon is arrested. Shrek and his friend Fudd are sent to reform school, while Aunt May takes on their father's job as a janitor after Aunt May passes away. Together, the three children visit various museums, learning to use the magic of their grandfather's house. When Shrek meets Fudd, they bond, and while they argue over Uncle Vernon's job and Uncle May's contract with the IRS, their friendship blossoms into love and together they find a new home. Meanwhile, Fudd is arrested again and sends to jail, while the children return home.\n",
      "\n",
      "\n",
      "------------------------\n",
      " <BOS> <superhero> Shrek and his friends Peter and pals are living happily, but Peter is depressed and Peter cannot accept it. Wandering along the streets of his fictional city of \"Rotten Milk\", he meets two children, two little girls, and a little boy named Finn. A small shopkeeper, Mr. Pyp, has been harassing children, since childhood. Peter and Finn, along with fellow toys, Hobo and Daffy, go to the shop, but find a child playing dead on the sidewalk. Thinking it may be the work of Mr. Pyp, they take the toys and leave. As they depart, Mr. Pyp makes several attempts to get his daughter back, but she is only interested in toys. Mr. Pyp then demands the toys back and demands that they return the body. When the toys refuse to return the body to him, he tries to escape. The toys are picked on by a local shopkeeper, Popsie, who claims to\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity check of pre-trained model\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "stories = generator(\"<BOS> <superhero> Shrek\", max_length=200, num_return_sequences=2)\n",
    "print(*[story['generated_text'] + \"\\n\\n\\n------------------------\\n\" for story in stories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52a7175d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <EOS> <|endoftext|> <PAD> <superhero> <action> <drama> <thriller> <horror> <sci_fi>\n"
     ]
    }
   ],
   "source": [
    "# Special tokens:\n",
    "print(*tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb793f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW TRAINING DATA SHOULD LOOK\n",
    "# The dataset should be a t\n",
    "\n",
    "[\"<BOS> <action> <drama> Inception <SEP> Plot text of inception <EOS>\",\n",
    " \"<BOS> <superhero> <action> <thriller> Batman begins <SEP> Plot text of Batman begins <EOS>\",\n",
    " \"<BOS> <sci_fi> <action> Star Wars <SEP> Plot text of Star Wards <EOS>\"] new_plot_text\n",
    "# we therefore must add special tokens <SEP> to separate title from plot \n",
    "# as well as tokens for each additional genre we want to support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c58f66e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <EOS> <|endoftext|> <PAD> <superhero> <action> <drama> <thriller> <horror> <sci_fi> <comedy> <SEP>\n"
     ]
    }
   ],
   "source": [
    "# Add additional special tokens\n",
    "special_tokens = tokenizer.additional_special_tokens\n",
    "special_tokens.extend(['<comedy>','<SEP>']) # TODO add all genres/keywords we want to allow\n",
    "new_special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "num_added_toks = tokenizer.add_special_tokens(new_special_tokens_dict)\n",
    "\n",
    "# We must resize token embeddings since new special tokens were added\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Special tokens:\n",
    "print(*tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3dfe06",
   "metadata": {},
   "source": [
    "# Create dataset\n",
    "\n",
    "First, we load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"text\", data_files={\"train\": \"train.txt\", \"validation\": \"validate.txt\"})\n",
    "print(datasets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b4e5f4",
   "metadata": {},
   "source": [
    "### Casual language modeling\n",
    "For causal language modeling (CLM) we are going to take all the texts in our dataset and concatenate them after they are tokenized. Then we will split them in examples of a certain sequence length. This way the model will receive chunks of contiguous text that may look like:\n",
    "    \n",
    "    part of text 1\n",
    "    \n",
    "or\n",
    "\n",
    "    end of text 1 <BOS_TOKEN> beginning of text 2\n",
    "    \n",
    " \n",
    "depending on whether they span over several of the original texts in the dataset or not.\n",
    "**Also the labels will be the same as the inputs, shifted to the left.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd530d9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "print(tokenizer.decode(tokenized_datasets[\"train\"][0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4aa37",
   "metadata": {},
   "source": [
    "Now for the harder part: we need to concatenate all our texts together then split the result in small chunks of a certain block_size. To do this, we will use the map method again, with the option batched=True. This option actually lets us change the number of examples in the datasets by returning a different number of examples than we got. This way, we can create our new samples from a batch of examples.\n",
    "First, we grab the maximum length our model was pretrained with. This might be a big too big to fit in our GPU RAM, so here we take a bit less at just 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15cf538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#block_size = tokenizer.model_max_length\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c99cf",
   "metadata": {},
   "source": [
    "Then we write the preprocessing function that will group our texts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f831e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce101ac",
   "metadata": {},
   "source": [
    "First note that we duplicate the inputs for our labels. This is because the model of the ðŸ¤— Transformers library apply the shifting to the right, so we don't need to do it manually.\n",
    "\n",
    "Also note that by default, the map method will send a batch of 1,000 examples to be treated by the preprocessing function. So here, we will drop the remainder to make the concatenated tokenized texts a multiple of block_size every 1,000 examples. You can adjust this behavior by passing a higher batch size (which will also be processed slower). You can also speed-up the preprocessing by using multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c50ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")\n",
    "print(lm_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1281049",
   "metadata": {},
   "source": [
    "And we can check our datasets have changed: now the samples contain chunks of block_size contiguous tokens, potentially spanning over several of our original texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(lm_datasets[\"train\"][0][\"input_ids\"]))\n",
    "print()\n",
    "print(tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd3f581",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned, we're ready to instantiate our Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"finetuning_test\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab54fe8b",
   "metadata": {},
   "source": [
    "The last argument to setup everything so we can push the model to the Hub regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook. If you want to save your model locally in a name that is different than the name of the repository it will be pushed, or if you want to push your model under an organization and not your name space, use the hub_model_id argument to set the repo name (it needs to be the full name, including your namespace: for instance \"sgugger/gpt-finetuned-wikitext2\" or \"huggingface/gpt-finetuned-wikitext2\").\n",
    "We pass along all of those to the Trainer class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5cc9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results=trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b09cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8bb30",
   "metadata": {},
   "source": [
    "### TODO\n",
    "Make inference (create a pipeline?) https://huggingface.co/transformers/task_summary.html#text-generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
