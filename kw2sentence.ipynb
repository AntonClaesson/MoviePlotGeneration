{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371c61c0",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "This notebook implements a pre-trained GPT language model to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c54df45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (4.11.3)\n",
      "Requirement already satisfied: datasets in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (1.12.1)\n",
      "Requirement already satisfied: git-lfs in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (1.6)\n",
      "Requirement already satisfied: multiprocess in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: pandas in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from datasets) (1.2.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from datasets) (2021.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: packaging in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from datasets) (20.9)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from datasets) (0.0.19)\n",
      "Requirement already satisfied: dill in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from datasets) (1.20.1)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: xxhash in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyyaml in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (5.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: sacremoses in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Requirement already satisfied: joblib in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /Users/antonclaesson/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e37d5e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /Users/antonclaesson/.huggingface/token\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd35e7",
   "metadata": {},
   "source": [
    "# START\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11eb35d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TextDataset,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "#from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aab1ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained tokenizer and model\n",
    "model_name = \"pranavpsv/gpt2-genre-story-generator\"\n",
    "config=AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,config=config)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c9fa17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <superhero> Shrek, a New York orphaned boy, and his Uncle Ben travel to Camp Crystal while Ben is away on a business trip from New York City. Upon arrival at Camp Crystal Ben is welcomed by Queen Hippolyta's daughter, Princess Evangeline; and the three visit the Fortress of Solitude for the annual Quidditch match. After much exploration of the Fortress Ben meets King Eobard Thordan, who falls in love with Ben and helps to secure an alliance between the two kingdoms. During the event, Ben and Evangeline are spotted by Thordan who plans to capture them for a reward.  Thordan captures Ben, the children, and Evangeline's maid, and forces Ben to take him to El Nino, a mountain pass, while the rest of Ben's group are captured by the rebels. Thordan also captures Evangeline, the Queen's new servant, and forces Ben and Ben's group to return home, but Evangeline\n",
      "\n",
      "\n",
      "------------------------\n",
      " <BOS> <superhero> Shrek is fighting the monster Magneto while he is training for the Battle of New York where he will fight a Super-X using Shrek`s own power. Meanwhile, Spider-Man has been attacked by the Black Widows. After saving Spider-Man from one of the group, he is teleported away with her body on Earth to a parallel dimension.While Shrek and the others arrive in their Earth dimension, Peter continues training him, but not without warning his younger sister, Zoe, from not wanting to see him with Spider-Man anymore. At school, Shrek and his friends meet with Kitty Pryde as she and her partner, the Wasp, discuss their adventures. Peter has problems with the Wasp controlling himself and the Wasp wants him to give up his power. Shrek is not very happy when his father, the Wasp, has a fight with the X-Men.  At home, his parents tell him that he will not fight\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity check of pre-trained model\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "stories = generator(\"<BOS> <superhero> Shrek\", max_length=200, num_return_sequences=2)\n",
    "print(*[story['generated_text'] + \"\\n\\n\\n------------------------\\n\" for story in stories])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3dfe06",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "\n",
    "First, we load the dataset and split into train and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "166ba740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>drama</td>\n",
       "      <td>19134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>comedy</td>\n",
       "      <td>10467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>romance film</td>\n",
       "      <td>6666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>c-movie</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>comdedy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>homoeroticism</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             genre  count\n",
       "9            drama  19134\n",
       "16          comedy  10467\n",
       "18    romance film   6666\n",
       "..             ...    ...\n",
       "111        c-movie      1\n",
       "286        comdedy      1\n",
       "362  homoeroticism      1\n",
       "\n",
       "[363 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display genres and count\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "data = pd.read_csv('data/genres.csv',names=['genre', 'count'])\n",
    "data=data.sort_values(by='count', ascending=False)\n",
    "pd.set_option('display.max_rows', 7)\n",
    "data.head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "766d392e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-aac4e4e1cce5f43e\n",
      "Reusing dataset text (/Users/antonclaesson/.cache/huggingface/datasets/text/default-aac4e4e1cce5f43e/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5fb1b3e78541f99115d29c7a76b4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /Users/antonclaesson/.cache/huggingface/datasets/text/default-aac4e4e1cce5f43e/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-fffe7ee87ffe1501.arrow and /Users/antonclaesson/.cache/huggingface/datasets/text/default-aac4e4e1cce5f43e/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-ed1c6d8aa0f9e101.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 40176\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1031\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from text file called \"data.txt\" and split into train/val\n",
    "datasets = load_dataset(\"text\", data_files=\"data.txt\")['train']\n",
    "datasets = datasets.train_test_split(train_size=0.975)\n",
    "datasets['validation'] = datasets.pop('test')\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7dc2b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <thriller> <melodrama> <adventure> <black-and-white> <drama> <romance film> Ceiling Zero <SEP> Old pals Jake Lee, Tex Clarke and Dizzy Davis flew together in the Army  during World War I. Almost twenty years later, Jake is the manager of the Newark, New Jersey branch of Federal Airlines, a New York-based airline company. Tex works as an airmail pilot and Dizzy, also still flying planes, is seeking employment with his friends. Prior to his hot-shot arrival , a New York associate warns Jake about Dizzy, calling him unreliable and troublesome. Insulted, Jake replies that Dizzy is one of the best pilots in the country, telling a few stories about his fearlessness and bravery. Jake hires Dizzy as an airmail pilot. Dizzy is immediately attracted to 'Tommy' Thomas, a 19 year old girl also working there, who has just learned to fly solo. In order to go on a date with her, Dizzy, scheduled for a flight to Cincinnati in the evening, pretends he is suddenly sick and gets Tex to replace him. Tex makes it to Ohio, but on the way back to New Jersey, finds himself in a cold and heavy fog. Though there is zero visibility and he is having radio problems, he attempts to land in Newark. He crashes into one of the airport hangars and the plane catches on fire. Tex is taken to the hospital where he later dies. Tex's wife Lou, who was never very fond of Dizzy, blames him for her husband's death. She calls him selfish and irresponsible and says that he hurts everything he touches. Dizzy, overwhelmed with guilt, returns to the airport. Meanwhile, the weather has gotten even worse and Jake has canceled all other flights. In addition, the aviation authorities have revoked Dizzy's pilot license, for extraneous reasons. Jake consoles Dizzy on account of both losses and then goes home for the night, leaving him temporarily in charge. Another pilot, unaware of the cancellation, comes into the operations building, ready for his normally scheduled flight. Chagrined and burdened with his culpability, Dizzy demands the man explain how the newly acquired and, as of yet, untested aircraft de-icers function, then knocks the man unconscious and irrationally takes his plane. Jake and the others are devastated when they find out. Dizzy radios information over to them about the de-icers. They work to a degree, but the system is flawed. He reports by radio on the problems of the system and his recommendations for modifications, knowing that he will watch progressive icing until he dies. He does not make it through the snow storm. <EOS>\n",
      "\n",
      "<BOS> <japanese movies> Liar Game: The Final Stage <SEP> Even though being able to withdraw from the Liar Game Tournament, Kanzaki Nao  decides to join Akiyama Shinichi  and nine other players for the final stage, ”The Garden of Eden game”, which offers a 5 billion yen prize. The players are informed that if all of them can work together, they can all become winners. However, the chances of winning are endangered by a mysterious player known as \"Person X\". <EOS>\n",
      "\n",
      "<BOS> <comedy> Forever Female <SEP> The reviews are in and a new play starring Beatrice Page and produced by Harry Phillips is a flop. Recently divorced but still a team, they need a new project and meet playwright Stanley Krown, who has written one in which the lead roles are a mother and a 19-year-old daughter. Beatrice wants to play the daughter. She can't pass for 19 but believes she can for 29, so wants the play rewritten. She also conveys a romantic interest in Stanley. A young actress first calling herself Sally Carver and then Polly Pruitt wants an audition. Stanley has her do some typing on his rewrite, and a jealous Beatrice finds her an acting job out of town. Stanley's play previews in Washington, D.C., and flops. Sally, now calling herself Claudia Souvain, tries to persuade Stanley that the actress is too old for the role. Seeing the play in a small town with Sally in the lead, now under her real name of Clara Mootz, convinces Stanley that she is right. Beatrice finally concedes that it's time for her to act her age. She agrees to take the mother's part, and on Broadway the play is a huge success. <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "print(datasets['train'][5]['text'] + '\\n')\n",
    "print(datasets['train'][6]['text'] + '\\n')\n",
    "print(datasets['train'][47]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30158d",
   "metadata": {},
   "source": [
    "As can be seen, the examples are of different lengths. Examples longer than 1024 tokens needs to be truncated as this is the maximum input to GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b94ce",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "We now need to tokenize the dataset. The pre-trained model that we are using have a few special tokens for a few genres. We need to add special tokens for all of our new genres as well as a special \\<SEP\\> token to the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b8e7fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SEP>', '<drama>', '<comedy>', '<romance film>', '<thriller>', '<action>', '<world cinema>', '<crime fiction>', '<horror>', '<black-and-white>']\n"
     ]
    }
   ],
   "source": [
    "new_special_tokens = ['<SEP>']\n",
    "new_special_tokens.extend(['<' + str(genre) + '>' for genre in data['genre']])\n",
    "print(new_special_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f09fb9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <EOS> <|endoftext|> <PAD> <superhero> <action> <drama> <thriller> <horror> <sci_fi> <SEP> <comedy> <romance film> <world cinema> <crime fiction> <black-and-white> <indie> <action/adventure> <adventure> <family film>\n"
     ]
    }
   ],
   "source": [
    "# Add new special tokens to the tokenizer \n",
    "special_tokens = tokenizer.additional_special_tokens\n",
    "special_tokens.extend(new_special_tokens) \n",
    "new_special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "num_added_toks = tokenizer.add_special_tokens(new_special_tokens_dict)\n",
    "\n",
    "# We must resize token embeddings since new special tokens were added\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Special tokens:\n",
    "print(*tokenizer.all_special_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e78fd",
   "metadata": {},
   "source": [
    "**Tokenize the dataset**\n",
    "\n",
    "We tokenize the dataset. The tokenized examples contain the column names 'attention_mask' which is a mask for padding tokens and 'input_ids' which is the id of each token corrsponding to a word. We drop the text as that is not needed anymore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd530d9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126bf81aeed644a2bb7896ba43daf60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6947a79d3193443d86f5d674b466a21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    padding='max_length' to pad to a length specified by the max_length argument \n",
    "    or the maximum length accepted by the model.\n",
    "    truncation=True to truncate to a maximum length accepted by the model\n",
    "    \"\"\"\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e3790f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids'],\n",
       "        num_rows: 40176\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'input_ids'],\n",
       "        num_rows: 1031\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make dataset format pytorch tensors\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50580649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, extract the datasets and select a subset if wanted\n",
    "train_set = tokenized_datasets['train'].select(list(range(2)))\n",
    "valid_set = tokenized_datasets['validation'].select(list(range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42e6e124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1]),\n",
       " 'input_ids': tensor([50257,   220, 50263,  ..., 22028,   323,   338])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab54fe8b",
   "metadata": {},
   "source": [
    "### Training\n",
    "First, setup training args.\n",
    "The last argument to setup everything so we can push the model to the Hub regularly during training..\n",
    "\n",
    "Then pass training args to Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79e0ae0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Cloning https://huggingface.co/AntonClaesson/movie-plot-generator into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "finetuned_model_name = \"movie-plot-generator\"\n",
    "training_args = TrainingArguments(\n",
    "    finetuned_model_name,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=valid_set,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5cc9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results=trainer.train()\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9873a5",
   "metadata": {},
   "source": [
    "### Push to HUB\n",
    "\n",
    "Push tokenizer and model to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e0c0bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./finetuned_model_name/tokenizer_config.json\n",
      "Special tokens file saved in ./finetuned_model_name/special_tokens_map.json\n",
      "tokenizer config file saved in movie-plot-generator/tokenizer_config.json\n",
      "Special tokens file saved in movie-plot-generator/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "On branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mgit_commit\u001b[0;34m(self, commit_message)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m             result = subprocess.run(\n\u001b[0m\u001b[1;32m    899\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0;34m\"git\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"commit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommit_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-v\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             raise CalledProcessError(retcode, process.args,\n\u001b[0m\u001b[1;32m    517\u001b[0m                                      output=stdout, stderr=stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['git', 'commit', '-m', 'add tokenizer', '-v']' returned non-zero exit status 1.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-4427c141b85e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./finetuned_model_name/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinetuned_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, repo_path_or_name, repo_url, use_temp_dir, commit_message, organization, private, use_auth_token)\u001b[0m\n\u001b[1;32m   2180\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_path_or_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m         \u001b[0;31m# Commit and push!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2182\u001b[0;31m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_push_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommit_message\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommit_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m         \u001b[0;31m# Clean up! Clean up! Everybody everywhere!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m_push_to_hub\u001b[0;34m(cls, repo, commit_message)\u001b[0m\n\u001b[1;32m   2260\u001b[0m                 \u001b[0mcommit_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"add model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrepo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommit_message\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommit_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, commit_message, blocking, clean_ok)\u001b[0m\n\u001b[1;32m   1191\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto_lfs_track\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit_commit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommit_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m         return self.git_push(\n\u001b[1;32m   1195\u001b[0m             \u001b[0mupstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"origin {self.current_branch}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mgit_commit\u001b[0;34m(self, commit_message)\u001b[0m\n\u001b[1;32m    909\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     def git_push(\n",
      "\u001b[0;31mOSError\u001b[0m: On branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\n"
     ]
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./finetuned_model_name/\")\n",
    "tokenizer.push_to_hub(finetuned_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b4e5f4",
   "metadata": {},
   "source": [
    "------\n",
    "------\n",
    "### Casual language modeling ## \n",
    "For causal language modeling (CLM) we are going to take all the texts in our dataset and concatenate them after they are tokenized. Then we will split them in examples of a certain sequence length. This way the model will receive chunks of contiguous text that may look like:\n",
    "    \n",
    "    part of text 1\n",
    "    \n",
    "or\n",
    "\n",
    "    end of text 1 <BOS_TOKEN> beginning of text 2\n",
    "    \n",
    " \n",
    "depending on whether they span over several of the original texts in the dataset or not.\n",
    "**Also the labels will be the same as the inputs, shifted to the left.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4aa37",
   "metadata": {},
   "source": [
    "Now for the harder part: we need to concatenate all our texts together then split the result in small chunks of a certain block_size. To do this, we will use the map method again, with the option batched=True. This option actually lets us change the number of examples in the datasets by returning a different number of examples than we got. This way, we can create our new samples from a batch of examples.\n",
    "First, we grab the maximum length our model was pretrained with. This might be a big too big to fit in our GPU RAM, in that case decrease the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15cf538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#block_size = tokenizer.model_max_length\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c99cf",
   "metadata": {},
   "source": [
    "Then we write the preprocessing function that will group our texts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f831e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce101ac",
   "metadata": {},
   "source": [
    "First note that we duplicate the inputs for our labels. This is because the model of the 🤗 Transformers library apply the shifting to the right, so we don't need to do it manually.\n",
    "\n",
    "Also note that by default, the map method will send a batch of 1,000 examples to be treated by the preprocessing function. So here, we will drop the remainder to make the concatenated tokenized texts a multiple of block_size every 1,000 examples. You can adjust this behavior by passing a higher batch size (which will also be processed slower). You can also speed-up the preprocessing by using multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c50ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")\n",
    "print(lm_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1281049",
   "metadata": {},
   "source": [
    "And we can check our datasets have changed: now the samples contain chunks of block_size contiguous tokens, potentially spanning over several of our original texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(lm_datasets[\"train\"][0][\"input_ids\"]))\n",
    "print()\n",
    "print(tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd3f581",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned, we're ready to instantiate our Trainer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
