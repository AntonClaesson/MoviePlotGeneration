{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd26678",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "This notebook implements a pre-trained GPT language model to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e454366d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (4.11.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (1.13.2)\n",
      "Requirement already satisfied: git-lfs in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (1.6)\n",
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-7.6.5-py2.py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages\\sacremoses-0.0.43-py3.8.egg (from transformers) (0.0.43)\n",
      "Requirement already satisfied: filelock in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from transformers) (0.0.19)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: dill in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from datasets) (1.3.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from datasets) (2021.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from ipywidgets) (5.1.0)\n",
      "Collecting nbformat>=4.2.0\n",
      "  Downloading nbformat-5.1.3-py3-none-any.whl (178 kB)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from ipywidgets) (0.2.0)\n",
      "Collecting widgetsnbextension~=3.5.0\n",
      "  Downloading widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2 MB)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: ipython>=4.0.0 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from ipywidgets) (7.27.0)\n",
      "Collecting jupyterlab-widgets>=1.0.0\n",
      "  Downloading jupyterlab_widgets-1.0.2-py3-none-any.whl (243 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from huggingface-hub>=0.0.17->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.4.1)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: jupyter-client<8.0 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (7.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (58.0.4)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (3.0.20)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.8.1)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.2.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from jupyter-core>=4.6.0->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (228)\n",
      "Collecting jsonschema!=2.5.0,>=2.4\n",
      "  Downloading jsonschema-4.1.0-py3-none-any.whl (69 kB)\n",
      "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
      "  Downloading pyrsistent-0.18.0-cp38-cp38-win_amd64.whl (62 kB)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Collecting notebook>=4.4.1\n",
      "  Downloading notebook-6.4.4-py3-none-any.whl (9.9 MB)\n",
      "Collecting argon2-cffi\n",
      "  Downloading argon2_cffi-21.1.0-cp35-abi3-win_amd64.whl (40 kB)\n",
      "Collecting Send2Trash>=1.5.0\n",
      "  Downloading Send2Trash-1.8.0-py3-none-any.whl (18 kB)\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.0.2-py3-none-any.whl (133 kB)\n",
      "Collecting terminado>=0.8.3\n",
      "  Downloading terminado-0.12.1-py3-none-any.whl (15 kB)\n",
      "Collecting prometheus-client\n",
      "  Downloading prometheus_client-0.11.0-py2.py3-none-any.whl (56 kB)\n",
      "Collecting nbconvert\n",
      "  Downloading nbconvert-6.2.0-py3-none-any.whl (553 kB)\n",
      "Collecting pywinpty>=1.1.0\n",
      "  Downloading pywinpty-1.1.4-cp38-none-win_amd64.whl (1.3 MB)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from aiohttp->datasets) (1.7.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.20)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.0.1-cp38-cp38-win_amd64.whl (14 kB)\n",
      "Collecting nbclient<0.6.0,>=0.5.0\n",
      "  Downloading nbclient-0.5.4-py3-none-any.whl (66 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting mistune<2,>=0.8.1\n",
      "  Using cached mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting defusedxml\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting testpath\n",
      "  Downloading testpath-0.5.0-py3-none-any.whl (84 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\n",
      "Collecting bleach\n",
      "  Downloading bleach-4.1.0-py2.py3-none-any.whl (157 kB)\n",
      "Collecting webencodings\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: click in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\anton\\anaconda3\\envs\\storygen\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Installing collected packages: pyrsistent, jsonschema, webencodings, nbformat, MarkupSafe, testpath, pywinpty, pandocfilters, nbclient, mistune, jupyterlab-pygments, jinja2, defusedxml, bleach, terminado, Send2Trash, prometheus-client, nbconvert, argon2-cffi, notebook, widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed MarkupSafe-2.0.1 Send2Trash-1.8.0 argon2-cffi-21.1.0 bleach-4.1.0 defusedxml-0.7.1 ipywidgets-7.6.5 jinja2-3.0.2 jsonschema-4.1.0 jupyterlab-pygments-0.1.2 jupyterlab-widgets-1.0.2 mistune-0.8.4 nbclient-0.5.4 nbconvert-6.2.0 nbformat-5.1.3 notebook-6.4.4 pandocfilters-1.5.0 prometheus-client-0.11.0 pyrsistent-0.18.0 pywinpty-1.1.4 terminado-0.12.1 testpath-0.5.0 webencodings-0.5.1 widgetsnbextension-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets git-lfs ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "914a8f38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at C:\\Users\\Anton\\Anaconda3\\envs\\storygen:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "abseil-cpp                20210324.2           h0e60522_0    conda-forge\n",
      "aiohttp                   3.7.4.post0      py38h294d835_0    conda-forge\n",
      "argon2-cffi               21.1.0                   pypi_0    pypi\n",
      "arrow-cpp                 5.0.0           py38h9929e98_8_cpu    conda-forge\n",
      "async-timeout             3.0.1                   py_1000    conda-forge\n",
      "attrs                     21.2.0             pyhd8ed1ab_0    conda-forge\n",
      "aws-c-cal                 0.5.11               he19cf47_0    conda-forge\n",
      "aws-c-common              0.6.2                h8ffe710_0    conda-forge\n",
      "aws-c-event-stream        0.2.7               h70e1b0c_13    conda-forge\n",
      "aws-c-io                  0.10.5               h2fe331c_0    conda-forge\n",
      "aws-checksums             0.1.11               h1e232aa_7    conda-forge\n",
      "aws-sdk-cpp               1.8.186              hb0612c5_3    conda-forge\n",
      "backcall                  0.2.0              pyhd3eb1b0_0  \n",
      "blas                      2.111                       mkl    conda-forge\n",
      "blas-devel                3.9.0              11_win64_mkl    conda-forge\n",
      "bleach                    4.1.0                    pypi_0    pypi\n",
      "boto3                     1.18.21            pyhd3eb1b0_0  \n",
      "botocore                  1.21.41            pyhd3eb1b0_1  \n",
      "brotlipy                  0.7.0           py38h294d835_1001    conda-forge\n",
      "bzip2                     1.0.8                h8ffe710_4    conda-forge\n",
      "c-ares                    1.17.2               h8ffe710_0    conda-forge\n",
      "ca-certificates           2021.10.8            h5b45459_0    conda-forge\n",
      "certifi                   2021.10.8        py38haa244fe_0    conda-forge\n",
      "cffi                      1.14.6           py38hd8c33c5_1    conda-forge\n",
      "chardet                   4.0.0            py38haa244fe_1    conda-forge\n",
      "click                     8.0.1              pyhd3eb1b0_0  \n",
      "colorama                  0.4.4              pyhd3eb1b0_0  \n",
      "cryptography              35.0.0           py38hd7da0ea_0    conda-forge\n",
      "cudatoolkit               11.1.1               heb2d755_9    conda-forge\n",
      "dataclasses               0.8                pyhc8e2a94_3    conda-forge\n",
      "datasets                  1.13.2                     py_0    huggingface\n",
      "debugpy                   1.4.1            py38hd77b12b_0  \n",
      "decorator                 5.1.0              pyhd3eb1b0_0  \n",
      "defusedxml                0.7.1                    pypi_0    pypi\n",
      "dill                      0.3.4              pyhd8ed1ab_0    conda-forge\n",
      "entrypoints               0.3                      py38_0  \n",
      "filelock                  3.3.0              pyhd8ed1ab_0    conda-forge\n",
      "freetype                  2.10.4               h546665d_1    conda-forge\n",
      "fsspec                    2021.10.0          pyhd8ed1ab_0    conda-forge\n",
      "gflags                    2.2.2             ha925a31_1004    conda-forge\n",
      "git-lfs                   1.6                      pypi_0    pypi\n",
      "glog                      0.5.0                h4797de2_0    conda-forge\n",
      "grpc-cpp                  1.40.0               h2431d41_2    conda-forge\n",
      "huggingface-hub           0.0.19                   pypi_0    pypi\n",
      "huggingface_hub           0.0.17                     py_0    huggingface\n",
      "idna                      2.10               pyh9f0ad1d_0    conda-forge\n",
      "importlib-metadata        4.8.1            py38haa244fe_0    conda-forge\n",
      "importlib_metadata        4.8.1                hd8ed1ab_0    conda-forge\n",
      "intel-openmp              2021.4.0          h57928b3_3556    conda-forge\n",
      "ipykernel                 6.4.1            py38haa95532_1  \n",
      "ipython                   7.27.0           py38hd4e2768_0  \n",
      "ipython_genutils          0.2.0              pyhd3eb1b0_1  \n",
      "ipywidgets                7.6.5                    pypi_0    pypi\n",
      "jbig                      2.1               h8d14728_2003    conda-forge\n",
      "jedi                      0.18.0           py38haa95532_1  \n",
      "jinja2                    3.0.2                    pypi_0    pypi\n",
      "jmespath                  0.10.0             pyhd3eb1b0_0  \n",
      "joblib                    1.0.1              pyhd3eb1b0_0  \n",
      "jpeg                      9d                   h8ffe710_0    conda-forge\n",
      "jsonschema                4.1.0                    pypi_0    pypi\n",
      "jupyter_client            7.0.1              pyhd3eb1b0_0  \n",
      "jupyter_core              4.8.1            py38haa95532_0  \n",
      "jupyterlab-pygments       0.1.2                    pypi_0    pypi\n",
      "jupyterlab-widgets        1.0.2                    pypi_0    pypi\n",
      "krb5                      1.19.2               hbae68bd_2    conda-forge\n",
      "lcms2                     2.12                 h2a16943_0    conda-forge\n",
      "lerc                      3.0                  h0e60522_0    conda-forge\n",
      "libblas                   3.9.0              11_win64_mkl    conda-forge\n",
      "libbrotlicommon           1.0.9                h8ffe710_5    conda-forge\n",
      "libbrotlidec              1.0.9                h8ffe710_5    conda-forge\n",
      "libbrotlienc              1.0.9                h8ffe710_5    conda-forge\n",
      "libcblas                  3.9.0              11_win64_mkl    conda-forge\n",
      "libcurl                   7.79.1               h789b8ee_1    conda-forge\n",
      "libdeflate                1.8                  h8ffe710_0    conda-forge\n",
      "liblapack                 3.9.0              11_win64_mkl    conda-forge\n",
      "liblapacke                3.9.0              11_win64_mkl    conda-forge\n",
      "libpng                    1.6.37               h1d00b33_2    conda-forge\n",
      "libprotobuf               3.18.1               h7755175_0    conda-forge\n",
      "libssh2                   1.10.0               h680486a_2    conda-forge\n",
      "libthrift                 0.15.0               h636ae23_1    conda-forge\n",
      "libtiff                   4.3.0                hd413186_2    conda-forge\n",
      "libutf8proc               2.6.1                hcb41399_0    conda-forge\n",
      "libuv                     1.42.0               h8ffe710_0    conda-forge\n",
      "libzlib                   1.2.11            h8ffe710_1013    conda-forge\n",
      "lz4-c                     1.9.3                h8ffe710_1    conda-forge\n",
      "m2w64-gcc-libgfortran     5.3.0                         6    conda-forge\n",
      "m2w64-gcc-libs            5.3.0                         7    conda-forge\n",
      "m2w64-gcc-libs-core       5.3.0                         7    conda-forge\n",
      "m2w64-gmp                 6.1.0                         2    conda-forge\n",
      "m2w64-libwinpthread-git   5.0.0.4634.697f757               2    conda-forge\n",
      "markupsafe                2.0.1                    pypi_0    pypi\n",
      "matplotlib-inline         0.1.2              pyhd3eb1b0_2  \n",
      "mistune                   0.8.4                    pypi_0    pypi\n",
      "mkl                       2021.3.0           hb70f87d_564    conda-forge\n",
      "mkl-devel                 2021.3.0           h57928b3_565    conda-forge\n",
      "mkl-include               2021.3.0           hb70f87d_564    conda-forge\n",
      "msys2-conda-epoch         20160418                      1    conda-forge\n",
      "multidict                 5.2.0            py38h294d835_0    conda-forge\n",
      "multiprocess              0.70.12.2        py38h294d835_0    conda-forge\n",
      "nbclient                  0.5.4                    pypi_0    pypi\n",
      "nbconvert                 6.2.0                    pypi_0    pypi\n",
      "nbformat                  5.1.3                    pypi_0    pypi\n",
      "nest-asyncio              1.5.1              pyhd3eb1b0_0  \n",
      "ninja                     1.10.2               h2d74725_1    conda-forge\n",
      "notebook                  6.4.4                    pypi_0    pypi\n",
      "numpy                     1.21.2           py38h089cfbf_0    conda-forge\n",
      "olefile                   0.46               pyh9f0ad1d_1    conda-forge\n",
      "openjpeg                  2.4.0                hb211442_1    conda-forge\n",
      "openssl                   1.1.1l               h8ffe710_0    conda-forge\n",
      "packaging                 21.0               pyhd8ed1ab_0    conda-forge\n",
      "pandas                    1.3.3            py38h5d928e2_0    conda-forge\n",
      "pandocfilters             1.5.0                    pypi_0    pypi\n",
      "parquet-cpp               1.5.1                         2    conda-forge\n",
      "parso                     0.8.2              pyhd3eb1b0_0  \n",
      "pickleshare               0.7.5           pyhd3eb1b0_1003  \n",
      "pillow                    8.3.2            py38h794f750_0    conda-forge\n",
      "pip                       21.2.2           py38haa95532_0  \n",
      "prometheus-client         0.11.0                   pypi_0    pypi\n",
      "prompt-toolkit            3.0.20             pyhd3eb1b0_0  \n",
      "pyarrow                   5.0.0           py38h9f6c39d_8_cpu    conda-forge\n",
      "pycparser                 2.20               pyh9f0ad1d_2    conda-forge\n",
      "pygments                  2.10.0             pyhd3eb1b0_0  \n",
      "pyopenssl                 21.0.0             pyhd8ed1ab_0    conda-forge\n",
      "pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge\n",
      "pyrsistent                0.18.0                   pypi_0    pypi\n",
      "pysocks                   1.7.1            py38haa244fe_3    conda-forge\n",
      "python                    3.8.12               h6244533_0  \n",
      "python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge\n",
      "python-xxhash             2.0.2            py38h294d835_0    conda-forge\n",
      "python_abi                3.8                      1_cp38    huggingface\n",
      "pytorch                   1.9.1           py3.8_cuda11.1_cudnn8_0    pytorch\n",
      "pytz                      2021.3             pyhd8ed1ab_0    conda-forge\n",
      "pywin32                   228              py38hbaba5e8_1  \n",
      "pywinpty                  1.1.4                    pypi_0    pypi\n",
      "pyyaml                    6.0                      pypi_0    pypi\n",
      "pyzmq                     22.2.1           py38hd77b12b_1  \n",
      "re2                       2021.09.01           h0e60522_0    conda-forge\n",
      "regex                     2021.8.3         py38h2bbff1b_0  \n",
      "requests                  2.25.1             pyhd3deb0d_0    conda-forge\n",
      "s3transfer                0.5.0              pyhd3eb1b0_0  \n",
      "sacremoses                master                     py_0    huggingface\n",
      "scikit-learn              1.0              py38h8224a6f_1    conda-forge\n",
      "scipy                     1.7.1            py38ha1292f7_0    conda-forge\n",
      "send2trash                1.8.0                    pypi_0    pypi\n",
      "sentencepiece             0.1.96                   pypi_0    pypi\n",
      "setuptools                58.0.4           py38haa95532_0  \n",
      "six                       1.16.0             pyh6c4a22f_0    conda-forge\n",
      "snappy                    1.1.8                ha925a31_3    conda-forge\n",
      "sqlite                    3.36.0               h2bbff1b_0  \n",
      "tbb                       2021.3.0             h2d74725_0    conda-forge\n",
      "terminado                 0.12.1                   pypi_0    pypi\n",
      "testpath                  0.5.0                    pypi_0    pypi\n",
      "threadpoolctl             3.0.0              pyh8a188c0_0    conda-forge\n",
      "tk                        8.6.11               h8ffe710_1    conda-forge\n",
      "tokenizers                0.10.3           py38h291c280_1    conda-forge\n",
      "torchaudio                0.9.1                      py38    pytorch\n",
      "torchvision               0.2.2                      py_3    pytorch\n",
      "tornado                   6.1              py38h2bbff1b_0  \n",
      "tqdm                      4.62.3                   pypi_0    pypi\n",
      "traitlets                 5.1.0              pyhd3eb1b0_0  \n",
      "transformers              4.11.3             pyhd8ed1ab_0    conda-forge\n",
      "typing-extensions         3.10.0.2             hd8ed1ab_0    conda-forge\n",
      "typing_extensions         3.10.0.2           pyha770c72_0    conda-forge\n",
      "urllib3                   1.26.7             pyhd8ed1ab_0    conda-forge\n",
      "vc                        14.2                 h21ff451_1  \n",
      "vs2015_runtime            14.27.29016          h5e58377_2  \n",
      "wcwidth                   0.2.5              pyhd3eb1b0_0  \n",
      "webencodings              0.5.1                    pypi_0    pypi\n",
      "wheel                     0.37.0             pyhd3eb1b0_1  \n",
      "widgetsnbextension        3.5.1                    pypi_0    pypi\n",
      "win_inet_pton             1.1.0            py38haa244fe_2    conda-forge\n",
      "wincertstore              0.2              py38haa95532_2  \n",
      "xxhash                    0.8.0                h8ffe710_3    conda-forge\n",
      "xz                        5.2.5                h62dcd97_1    conda-forge\n",
      "yaml                      0.2.5                he774522_0    conda-forge\n",
      "yarl                      1.7.0            py38h294d835_0    conda-forge\n",
      "zipp                      3.6.0              pyhd8ed1ab_0    conda-forge\n",
      "zlib                      1.2.11            h8ffe710_1013    conda-forge\n",
      "zstd                      1.5.0                h6255e5f_0    conda-forge\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fa284b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to C:\\Users\\Anton/.huggingface/token\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba8935",
   "metadata": {},
   "source": [
    "# START\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a559a305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e04487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained tokenizer and model\n",
    "model_name = \"pranavpsv/gpt2-genre-story-generator\"\n",
    "config=AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,config=config)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e16326c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <superhero> Shrek and Queen Cadia are hunting an alternate version of the Green Goblin, who have taken over the Green Goblin Kingdom, during the \"Dark Hour\". Upon arriving at the Kingdom, Shrek and Queen Cadia are greeted by King Arthur's granddaughter, Princess Peacock. After they have eaten some ice cream, Peacock begins cursing to be sacrificed, but King Arthur suddenly changes into another Green Goblin disguise and they begin to battle, with the Queen succeeding with their combined strength allowing Shrek and Queen Cadia to escape to their kingdom. When King Arthur arrives, he is confronted by the Dark Hour minions in disguise and the villains are forced to flee through the kingdom gate.  Elsewhere, the King and Queen are informed by their father, Prince Charming, about a prophecy that told them that they would be granted eternal life in exchange for granting him wishes (the promise is implied to be an oracle). One of the Dark Hour minions, a dark witch, sacrifices\n",
      "\n",
      "\n",
      "------------------------\n",
      " <BOS> <superhero> Shrek, a child born and raised on Earth, is rescued at birth by Donkey and the rest of the Daffyks, a race of aliens whose ship has been destroyed. The ship is destroyed, but Shrek continues his journey to find the family, including Donkey's niece Lillian.\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity check of pre-trained model\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "stories = generator(\"<BOS> <superhero> Shrek\", max_length=200, num_return_sequences=2)\n",
    "print(*[story['generated_text'] + \"\\n\\n\\n------------------------\\n\" for story in stories])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d54c69",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "\n",
    "First, we load the dataset and split into train and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a465cdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>drama</td>\n",
       "      <td>19134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>comedy</td>\n",
       "      <td>10467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>romance film</td>\n",
       "      <td>6666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>c-movie</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>comdedy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>homoeroticism</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             genre  count\n",
       "9            drama  19134\n",
       "16          comedy  10467\n",
       "18    romance film   6666\n",
       "..             ...    ...\n",
       "111        c-movie      1\n",
       "286        comdedy      1\n",
       "362  homoeroticism      1\n",
       "\n",
       "[363 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display genres and count\n",
    "data = pd.read_csv('data/genres.csv',names=['genre', 'count'])\n",
    "data=data.sort_values(by='count', ascending=False)\n",
    "pd.set_option('display.max_rows', 7)\n",
    "data.head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c379c5e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-94f8bf6e17e9d4f9\n",
      "Reusing dataset text (C:\\Users\\Anton\\.cache\\huggingface\\datasets\\text\\default-94f8bf6e17e9d4f9\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4bf6844e3e4c1e961f7aed35e071d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\Anton\\.cache\\huggingface\\datasets\\text\\default-94f8bf6e17e9d4f9\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-019ae2e90bbe9e93.arrow and C:\\Users\\Anton\\.cache\\huggingface\\datasets\\text\\default-94f8bf6e17e9d4f9\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-669fe46fd39678c0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 40176\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1031\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from text file called \"data.txt\" and split into train/val\n",
    "datasets = load_dataset(\"text\", data_files=\"data.txt\")['train']\n",
    "datasets = datasets.train_test_split(train_size=0.975)\n",
    "datasets['validation'] = datasets.pop('test')\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "490cccae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <thriller> <crime fiction> <melodrama> <world cinema> <musical> <drama> <suspense> <crime thriller> <bollywood> Baazigar <SEP> Ajay Sharma , is a man who seeks revenge. His father, Vishwanath Sharma , once owner of a great business empire, was defrauded by Madan Chopra , a trusted employee in Vishwanath's company. The Sharma family is ousted from their own company and loses everything they owned. Soon afterwards, Ajay's father and his young baby sister die due to illness, unable to buy medicine because of their destitute state. His mother  is now suffering from mental illness and memory loss. Driven over the edge, Ajay becomes obsessed with killing Chopra and destroying his family. Ajay begins dating Seema Chopra , the daughter of the owner of Chopra business empire. They meet secretly as her father would not approve of a poor son-in-law. Meanwhile, the younger daughter Priya Chopra  travels with her father Madan Chopra to Madras , for Madan's final kart race before he retires permanently. Madan has never lost before, but comes across Vicky Malhotra . Vicky allows Madan to win by slowing at the last corner and tells him that he couldn't beat his \"guru\". Vicky then charms Priya by saying that he lost the race as he couldn't break the heart of a beautiful girl. Thus his ploy of winning in spite of losing  succeeds as he wins Priya's heart. This way, he manages to date both Seema and Priya simultaneously using different identities. Ajay is  photographed lurking outside the birthday party of Seema by one of her friends. Later Madan Chopra arranges for Seema to be married off to another business family. Seema is heartbroken, and Ajay decides that they will write identical suicide notes and commit suicide. After they each write their suicide notes, he tells her that the suicide note was just a test. He says that only cowards commit suicide and destroys his note, while keeping Seema's. They decide to marry secretly the next day. When they arrive, the registrar is closed, so they go up to the roof of the building for sightseeing. He makes her sit on the parapet and tells her the truth about himself, after which he throws her to her death from the rooftop of the building. He then posts the suicide note and leaves, but arrives later  with Priya. He helps the Chopra family with the funeral. The letter written by Seema implies that she has committed suicide, and the murder investigation is closed. Priya cannot believe that, and asks her father to re-open the case. However, he disagrees, saying he doesn't want to lose his reputation due to Seema's affair. Priya then asks Karan , who is her former classmate and a current police inspector, for help. Priya tries to investigate Seema's death, however her attempts prove unsuccessful. Ravi, Seema's college friend, offers to help her, but is brutally killed by Vicky. As Vicky forces him to sign a suicide note before hanging him, Karan thinks that Ravi must have been in love with Seema and hence her murderer. Later, Priya and Vicky meet Seema's college friend, Anjali, who becomes suspicious of him. When she finds out that he was Seema's boyfriend back in college, she calls the Sharma household during Vicky's and Priya's engagement party. Vicky intercepts the phone and impersonates Chopra, and then arrives at her place. He strangles her, then fits her body in a suitcase and throws it in the river. Vicky vows to destroy Madan Chopra, confiscate his assets and throw him out of his office and onto the street, just like Chopra did many years ago to Ajay's father. Chopra needs to go on a business trip, and knowing that Vicky's his soon to be son-in-law, gives Vicky the Power of attorney. Vicky does a carbon copy of the previous montage of Chopra, and within days seizes everything. One day in a club Vicky takes Priya to calm her mind, where they meet Ajay's childhood friend, who greets him as Ajay. While Vicky tries to ignore him, Priya tells the man that he is mistaken, and that Ajay is Vicky Malhotra. Ajay's friend repeatedly tries to remind Vicky  about their friendship. Vicky feels trapped and tries to get out of the mess by abusing him and gets in a scuffle with him. Priya visits Ajay's friend and the whole truth is revealed to her. When Chopra returns, Vicky reveals himself as Vishwanath Sharma's son Ajay and gives him the exact sendoff that Chopra did 15–20 years ago. Priya finds Ajay's home address, and finds out his real identity. Ajay comes in and they argue about Seema's murder. Ajay wins her over by telling her the history of their two families. Madan charges in with a group of thugs and shoots Ajay in the shoulder before he is severely beaten and wounded. Ajay's Mother is then knocked unconscious trying to defend Ajay. This prompts him to recover and fight Madan and his gang all alone. Chopra eventually picks up a stake and mortally stabs Ajay. However, Ajay manages to impale him with the same stake by walking into him, and they jump off a small cliff. Chopra dies, and Ajay  manages to stagger back to his mother. He tells her that he has reclaimed what is rightfully hers. Mrs. Sharma is cured of her illness, but is beset by sorrow, along with Priya crying beside her, as Ajay dies in their arms. <EOS>\n",
      "\n",
      "<BOS> <thriller> <media satire> <mystery> <crime fiction> <comedy> <crime thriller> <whodunit> The Scarlet Clue <SEP> Charlie Chan and his son Tommy investigate the shenanigans that results from a furtive gang trying to steal radar secrets from a radio station. <EOS>\n",
      "\n",
      "<BOS> <musical> <drama> <comedy film> Sughamo devi <SEP> Sukhamo Devi is the story of Nandan and Thara getting married as their personal love affairs failed. Nandan was in love with Devi, but circumstances forces Devi to choose another person. Thara was in love with Sunny, but he gets killed in an accident. The film also show the good side of friendship between Sunny and Vinod, who survives the accident, but living his a mechanical life. The film was a treat for campus lovers. <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "print(datasets['train'][0]['text'] + '\\n')\n",
    "print(datasets['train'][1]['text'] + '\\n')\n",
    "print(datasets['train'][2]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f79271",
   "metadata": {},
   "source": [
    "As can be seen, the examples are of different lengths. Examples longer than 1024 tokens needs to be truncated as this is the maximum input to GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbb865",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "We now need to tokenize the dataset. The pre-trained model that we are using have a few special tokens for a few genres. We need to add special tokens for all of our new genres as well as a special \\<SEP\\> token to the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd7d6f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SEP>', '<drama>', '<comedy>', '<romance film>', '<thriller>', '<action>', '<world cinema>', '<crime fiction>', '<horror>', '<black-and-white>']\n"
     ]
    }
   ],
   "source": [
    "new_special_tokens = ['<SEP>']\n",
    "new_special_tokens.extend(['<' + str(genre) + '>' for genre in data['genre']])\n",
    "print(new_special_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac15f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <EOS> <|endoftext|> <PAD> <superhero> <action> <drama> <thriller> <horror> <sci_fi> <SEP> <comedy> <romance film> <world cinema> <crime fiction> <black-and-white> <indie> <action/adventure> <adventure> <family film>\n"
     ]
    }
   ],
   "source": [
    "# Add new special tokens to the tokenizer \n",
    "special_tokens = tokenizer.additional_special_tokens\n",
    "special_tokens.extend(new_special_tokens) \n",
    "new_special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "num_added_toks = tokenizer.add_special_tokens(new_special_tokens_dict)\n",
    "\n",
    "# We must resize token embeddings since new special tokens were added\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Special tokens:\n",
    "print(*tokenizer.all_special_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d6e30",
   "metadata": {},
   "source": [
    "**Tokenize the dataset**\n",
    "\n",
    "We tokenize the dataset. The tokenized examples contain the column names 'attention_mask' which is a mask for padding tokens and 'input_ids' which is the id of each token corrsponding to a word. We drop the text as that is not needed anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22681185",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bace05b76d6c4bfd99a3edcebd73d8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Anton\\.cache\\huggingface\\datasets\\text\\default-94f8bf6e17e9d4f9\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-8cca187c8a34f2b7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    padding='max_length' to pad to a length specified by the max_length argument \n",
    "    or the maximum length accepted by the model.\n",
    "    truncation=True to truncate each sequence to the maximum length accepted by the model\n",
    "    \"\"\"\n",
    "    result = tokenizer(examples[\"text\"], padding='max_length', truncation=True)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f618dc0",
   "metadata": {},
   "source": [
    "Note that we duplicate the inputs to add our labels. This is because the model of the 🤗 Transformers library apply the shifting to the right, so we don't need to do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8713cdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels'],\n",
       "        num_rows: 40176\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels'],\n",
       "        num_rows: 1031\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make dataset format pytorch tensors\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbd91dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, extract the datasets and select a subset if wanted\n",
    "train_set = tokenized_datasets['train'].select(list(range(200)))\n",
    "valid_set = tokenized_datasets['validation'].select(list(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "002c34dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 200\n",
      "}) Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_set, valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fefa121",
   "metadata": {},
   "source": [
    "### Training\n",
    "First, setup training args.\n",
    "The last argument to setup everything so we can push the model to the Hub regularly during training..\n",
    "\n",
    "Then pass training args to Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ce3eb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton\\Desktop\\Chalmers\\ssy340\\MoviePlotGeneration\\movie-plot-generator is already a clone of https://huggingface.co/AntonClaesson/movie-plot-generator. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "finetuned_model_name = \"movie-plot-generator\"\n",
    "training_args = TrainingArguments(\n",
    "    finetuned_model_name,\n",
    "    evaluation_strategy = \"no\",\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=1,\n",
    "    push_to_hub=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=valid_set,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf3df559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 200\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.54 GiB (GPU 0; 8.00 GiB total capacity; 4.31 GiB already allocated; 1.51 GiB free; 4.81 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12056/3209807192.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_results\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0meval_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\storygen\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2112\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2113\u001b[1;33m         output = eval_loop(\n\u001b[0m\u001b[0;32m   2114\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2115\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Evaluation\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\storygen\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2284\u001b[0m             \u001b[1;31m# Prediction step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2285\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2287\u001b[0m             \u001b[1;31m# Update containers on host\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\storygen\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m   2487\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2488\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2489\u001b[1;33m                     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2490\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2491\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\storygen\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   1879\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1881\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1882\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1883\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\storygen\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\storygen\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m             \u001b[1;31m# Shift so that tokens < n predict n\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m             \u001b[0mshift_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlm_logits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m             \u001b[0mshift_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m             \u001b[1;31m# Flatten the tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.54 GiB (GPU 0; 8.00 GiB total capacity; 4.31 GiB already allocated; 1.51 GiB free; 4.81 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "train_results=trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d7364",
   "metadata": {},
   "source": [
    "### Push to HUB\n",
    "\n",
    "Push tokenizer and model to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c608d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./finetuned_model_name/tokenizer_config.json\n",
      "Special tokens file saved in ./finetuned_model_name/special_tokens_map.json\n",
      "tokenizer config file saved in movie-plot-generator\\tokenizer_config.json\n",
      "Special tokens file saved in movie-plot-generator\\special_tokens_map.json\n",
      "To https://huggingface.co/AntonClaesson/movie-plot-generator\n",
      "   03f5323..5dd6bdb  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/AntonClaesson/movie-plot-generator/commit/5dd6bdb7551d31c6264df6003a9bae7a62a056b8'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./finetuned_model_name/\")\n",
    "tokenizer.push_to_hub(finetuned_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b756a378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to movie-plot-generator\n",
      "Configuration saved in movie-plot-generator\\config.json\n",
      "Model weights saved in movie-plot-generator\\pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c64f96044e44accb65ddcfce400af1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Upload file pytorch_model.bin', max=511534313.0, style=Pr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f683edde66c842599e0d8134cd62aaa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Upload file training_args.bin', max=2799.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/AntonClaesson/movie-plot-generator\n",
      "   5dd6bdb..a5b4dab  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping the following result as it does not have all the necessary field:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "To https://huggingface.co/AntonClaesson/movie-plot-generator\n",
      "   a5b4dab..72fbc7d  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/AntonClaesson/movie-plot-generator/commit/a5b4dabc61120b768cf25330e1d14a20184ea655'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094df378",
   "metadata": {},
   "source": [
    "------\n",
    "------\n",
    "### Casual language modeling ## \n",
    "For causal language modeling (CLM) we are going to take all the texts in our dataset and concatenate them after they are tokenized. Then we will split them in examples of a certain sequence length. This way the model will receive chunks of contiguous text that may look like:\n",
    "    \n",
    "    part of text 1\n",
    "    \n",
    "or\n",
    "\n",
    "    end of text 1 <BOS_TOKEN> beginning of text 2\n",
    "    \n",
    " \n",
    "depending on whether they span over several of the original texts in the dataset or not.\n",
    "**Also the labels will be the same as the inputs, shifted to the left.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea116cb2",
   "metadata": {},
   "source": [
    "Now for the harder part: we need to concatenate all our texts together then split the result in small chunks of a certain block_size. To do this, we will use the map method again, with the option batched=True. This option actually lets us change the number of examples in the datasets by returning a different number of examples than we got. This way, we can create our new samples from a batch of examples.\n",
    "First, we grab the maximum length our model was pretrained with. This might be a big too big to fit in our GPU RAM, in that case decrease the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0410fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#block_size = tokenizer.model_max_length\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5821617",
   "metadata": {},
   "source": [
    "Then we write the preprocessing function that will group our texts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508479f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905bd446",
   "metadata": {},
   "source": [
    "First note that we duplicate the inputs for our labels. This is because the model of the 🤗 Transformers library apply the shifting to the right, so we don't need to do it manually.\n",
    "\n",
    "Also note that by default, the map method will send a batch of 1,000 examples to be treated by the preprocessing function. So here, we will drop the remainder to make the concatenated tokenized texts a multiple of block_size every 1,000 examples. You can adjust this behavior by passing a higher batch size (which will also be processed slower). You can also speed-up the preprocessing by using multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb277ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")\n",
    "print(lm_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60be398d",
   "metadata": {},
   "source": [
    "And we can check our datasets have changed: now the samples contain chunks of block_size contiguous tokens, potentially spanning over several of our original texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e003957",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(lm_datasets[\"train\"][0][\"input_ids\"]))\n",
    "print()\n",
    "print(tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399c21f",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned, we're ready to instantiate our Trainer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storygen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
