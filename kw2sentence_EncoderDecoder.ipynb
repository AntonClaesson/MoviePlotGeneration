{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cba8935",
   "metadata": {},
   "source": [
    "# START\n",
    "\n",
    "This notebook utilizes an encoder-decoder architecture to do movie plot generation from keywords in a sequence-to-sequence fashion.\n",
    "https://huggingface.co/transformers/model_doc/bertgeneration.html\n",
    "\n",
    "https://arxiv.org/pdf/1907.12461.pdf finds that combining BERT2GPT does not work presumably because the model needs to learn two vocabs. Note that \"Since RoBERTa and GPT-2 share the same vocabulary, combining both into a single model (ROBERTA2GPT) showed strong results on several tasks\". -> Let's do RoBertA to GPT2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a559a305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    TrainerCallback,\n",
    "    GPT2Config,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    AdamW,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from transformers import BartTokenizer, BartTokenizerFast, BartModel, BartForConditionalGeneration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b98c415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd5717664a14cb0ba76b08213383ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd119c2766a34ed19c3e6fca1b7b51d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bed0545d88146238575d0ace2f06147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab7d62df1f346a39290766f9474065b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f6a001115748859e69522c2992f578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/532M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetuned_model_name = 'BART-movie-plot-generator'\n",
    "tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-base')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb6fb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "</s>\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token)\n",
    "print(tokenizer.eos_token)\n",
    "print(tokenizer.sep_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22165f5b",
   "metadata": {},
   "source": [
    "We need to change the dataset tokens to fit the pre-trained tokenizer tokens and add tokens for all genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d54c69",
   "metadata": {},
   "source": [
    "# Load and process dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c379c5e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ede872a38110cd1a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /Users/antonclaesson/.cache/huggingface/datasets/text/default-ede872a38110cd1a/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ce4dc14646467094a3ad9419103ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b331380530104b3395fbc93447fd5323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /Users/antonclaesson/.cache/huggingface/datasets/text/default-ede872a38110cd1a/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16997de1724f43fd9cc173286547e307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7dd0732ed14c0b90e19f583166ca53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 37031\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from text file called \"data.txt\" and split into train/val\n",
    "dataset = load_dataset(\"text\", data_files=\"data_top_15_genres.txt\")['train']\n",
    "\n",
    "def processText(example):\n",
    "    example['text'] = [ re.sub('<BOS>', '<s>', text) for text in example['text'] ]\n",
    "    example['text'] = [ re.sub('<EOS>', '</s>', text) for text in example['text'] ]\n",
    "    example['text'] = [ re.sub('<SEP>', '</s>', text) for text in example['text'] ]    \n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(processText, batched=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "047bc04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> <thriller> <horror> <adventure> <action> Ghosts of Mars </s> Set in the second half of the 22nd century, the film depicts Mars as a planet that has been 84% terraformed, allowing humans to walk on the surface without wearing pressure suits. The Martian society has become largely matriarchal, with women in most positions of authority. The story concerns a police officer, Melanie Ballard , second in command of a small team alongside Sergeant Jericho  sent to pick up and transport a prisoner named Desolation Williams . Arriving at the remote mining town where Williams is being held, Ballard finds virtually all of the people missing. She learns that the miners had discovered an underground doorway created by an ancient Martian civilization. When the door was opened it released \"ghosts,\" disembodied spirits which possessed the miners. Violence ensues, as the possessed miners commit horrific acts of death and destruction, as well as self-mutilation. With their team leader Helena Bradock  murdered, Ballard must fight off the attacking miners, escape the town, and destroy the ghosts, if possible. Unfortunately, her intentions are complicated by the fact that killing a possessed human merely releases the Martian spirit to possess another human. The team eventually decides to blow up a nuclear reactor to try and vaporize all of the ghosts. At several points in the film Sergeant Jericho shows a romantic interest in Ballard, mostly unreciprocated. Ballard\\'s crew along with survivors who manage to gather in the jail are eventually wiped out by the miners after many fierce battles and events , leaving only her and Williams after Sergeant Jericho and the other remaining officers and the two operators of the train are killed upon returning from a brief retreat to finish the fight. Not wanting the authorities to blame the massacre on him, he handcuffs Ballard to her cot and escapes from the train, leaving her to return home and deliver her report, which is received with skepticism by her superiors. While Ballard recuperates at a hospital, the released spirits, who weren\\'t destroyed after all, attack the city. The end scene sets the movie up for a sequel as Williams returns to team up with Ballard to fight the possessed. </s>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbb865",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "We now need to tokenize the dataset. The original tokenizer don't have all special tokens we require."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d5c9e",
   "metadata": {},
   "source": [
    "We need to add the special tokens that we use in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ac15f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of added genres: 15\n",
      "50280 50280\n",
      "<s> </s> <unk> <pad> <mask> <romantic drama> <short film> <family film> <adventure> <action/adventure> <indie> <black-and-white> <horror> <crime fiction> <world cinema> <action> <thriller> <romance film> <comedy> <drama>\n"
     ]
    }
   ],
   "source": [
    "# Add special tokens for each genre\n",
    "genres = ['romantic drama', 'short film', 'family film',\n",
    "          'adventure', 'action/adventure', 'indie',\n",
    "          'black-and-white', 'horror', 'crime fiction',\n",
    "          'world cinema', 'action', 'thriller', \n",
    "          'romance film', 'comedy', 'drama']\n",
    "\n",
    "print(f'Number of added genres: {len(genres)}')\n",
    "special_tokens_dict = {'additional_special_tokens': [f'<{genre}>' for genre in genres]}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# We must resize token embeddings since new special tokens were added\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(model.config.vocab_size, tokenizer.vocab_size + len(tokenizer.get_added_vocab()))\n",
    "assert(model.config.vocab_size == tokenizer.vocab_size + len(tokenizer.get_added_vocab()))\n",
    "print(*tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "853e6b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Ġ',\n",
       " '<drama>',\n",
       " 'ĠThis',\n",
       " 'Ġis',\n",
       " 'Ġthe',\n",
       " 'Ġtitle',\n",
       " 'Ġ',\n",
       " '</s>',\n",
       " 'Ġhere',\n",
       " 'Ġis',\n",
       " 'Ġthe',\n",
       " 'Ġplot',\n",
       " 'Ġ',\n",
       " '</s>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('<s> <drama> This is the title </s> here is the plot </s>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d6e30",
   "metadata": {},
   "source": [
    "**Tokenize the dataset**\n",
    "\n",
    "We tokenize the dataset. The tokenized examples contain the column names 'attention_mask' which is a mask for padding tokens and 'input_ids' which is the id of each token corrsponding to a word. We drop the text as that is not needed anymore. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f618dc0",
   "metadata": {},
   "source": [
    "Note that we duplicate the inputs to add our labels. This is because the model of the 🤗 Transformers library apply the shifting to the right, so we don't need to do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22681185",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4de11ff6254b30af49e39794909943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    padding='max_length' to pad to a length specified by the max_length argument \n",
    "    or the maximum length accepted by the model.\n",
    "    truncation=True to truncate each sequence to the maximum length accepted by the model\n",
    "    \"\"\"\n",
    "    result = tokenizer(examples[\"text\"], padding='max_length', truncation=True) # Max input according to model(1024)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "#Make dataset format pytorch tensors\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f555394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'labels'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select subset if wanted\n",
    "train_set = tokenized_dataset.select(list(range(10)))\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fefa121",
   "metadata": {},
   "source": [
    "### Training\n",
    "First, setup training args.\n",
    "The last argument to setup everything so we can push the model to the Hub regularly during training..\n",
    "\n",
    "Then pass training args to Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e6034ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveTokenizer(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback used to save the tokenizer whenever a model checkpoint is saved.\n",
    "    \"\"\"\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        tokenizer.save_pretrained(finetuned_model_name)\n",
    "\n",
    "        \n",
    "ce_loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    The compute function needs to receive a tuple (with logits and labels)\n",
    "    and has to return a dictionary with string keys (the name of the metric) and float values.\n",
    "    It will be called at the end of each evaluation phase on the whole arrays of predictions/labels.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    # Calculate perplexity https://huggingface.co/transformers/perplexity.html\n",
    "    # \"the exponentiation of the cross-entropy between the data and model predictions.\"\n",
    "    \n",
    "    perplexity = math.exp(ce_loss(logits, labels))\n",
    "    \n",
    "    return {'perplexity': perplexity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce3eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "batch_size = 2 # 1:34:39 for one epoch (no evaluation steps) with batch_size = 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    finetuned_model_name,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    save_steps=2500,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=valid_set,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[SaveTokenizer],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3df559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_results=trainer.train()\n",
    "pickle.dump(train_results, open(finetuned_model_name+\"/train_results.pickle\", \"wb\")) #Load: train_results = pickle.load(open(\"train_results.pickle\", \"rb\"))\n",
    "model.save_pretrained(finetuned_model_name)\n",
    "tokenizer.save_pretrained(finetuned_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89be0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference test\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "stories = generator(\"<s> <horror> Testing <\\s>\", max_length=1024, num_return_sequences=4)\n",
    "print(*[story['generated_text'] + \"\\n\\n\\n------------------------\\n\" for story in stories])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
