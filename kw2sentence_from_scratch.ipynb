{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a559a305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    TrainerCallback,\n",
    "    GPT2Config,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    AdamW,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from transformers import (GPT2Config,\n",
    "                          GPT2LMHeadModel)\n",
    "\n",
    "model_name = 'movie-plot-generation-from-scratch'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d54c69",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "\n",
    "First, we load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c379c5e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2fcf8d2135508f85\n",
      "Reusing dataset text (C:\\Users\\Anton\\.cache\\huggingface\\datasets\\text\\default-2fcf8d2135508f85\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63f938351cc4bb5adc8617b6d5d011e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 37031\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from text file called \"data.txt\". We won't use a validation set\n",
    "dataset = load_dataset(\"text\", data_files=\"data_top_15_genres.txt\")['train']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbb865",
   "metadata": {},
   "source": [
    "## Tokenizer training\n",
    "\n",
    "We now need to tokenize the dataset. We create a tokenizer and train it on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82b4830d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('movie-plot-generation-from-scratch\\\\tokenizer_config.json',\n",
       " 'movie-plot-generation-from-scratch\\\\special_tokens_map.json',\n",
       " 'movie-plot-generation-from-scratch\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add special tokens for each genre\n",
    "genres = ['romantic drama', 'short film', 'family film',\n",
    "          'adventure', 'action/adventure', 'indie',\n",
    "          'black-and-white', 'horror', 'crime fiction',\n",
    "          'world cinema', 'action', 'thriller', \n",
    "          'romance film', 'comedy', 'drama']\n",
    "\n",
    "special_tokens = ['<UNK>', '<BOS>', '<EOS>', '<PAD>', '<SEP>']\n",
    "genre_tokens =  [f'<{genre}>' for genre in genres]\n",
    "all_special_tokens = special_tokens + genre_tokens\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"<UNK>\"))\n",
    "trainer = BpeTrainer(special_tokens=all_special_tokens, vocab_size=50257)\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.train_from_iterator(dataset['text'], trainer)\n",
    "\n",
    "# load our tokenizer into huggingface transformers library \n",
    "# For some reason the special tokens are not assigned to the corresponding properties \n",
    "# even though tokenization works as intended. We therefore add the special tokens manually.\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer, \n",
    "    model_input_names=['input_ids', 'attention_mask'])\n",
    "special_tokens_dict = {'additional_special_tokens': genre_tokens}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "tokenizer.unk_token = '<UNK>'\n",
    "tokenizer.bos_token = '<BOS>'\n",
    "tokenizer.eos_token = '<EOS>'\n",
    "tokenizer.pad_token = '<PAD>'\n",
    "tokenizer.sep_token = '<SEP>'\n",
    "\n",
    "# Save \n",
    "tokenizer.save_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9d7f5",
   "metadata": {},
   "source": [
    "### Define transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b2cb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='movie-plot-generation-from-scratch', vocab_size=50257, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'bos_token': '<BOS>', 'eos_token': '<EOS>', 'unk_token': '<UNK>', 'sep_token': '<SEP>', 'pad_token': '<PAD>', 'additional_special_tokens': ['<romantic drama>', '<short film>', '<family film>', '<adventure>', '<action/adventure>', '<indie>', '<black-and-white>', '<horror>', '<crime fiction>', '<world cinema>', '<action>', '<thriller>', '<romance film>', '<comedy>', '<drama>']})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a new GPT2 model with 512 max length\n",
    "config = GPT2Config(\n",
    "    vocab_size=50257,\n",
    "    n_positions=512,\n",
    "    n_ctx=512,\n",
    ")\n",
    "model = GPT2LMHeadModel(config=config)\n",
    "\n",
    "# Load tokenizer \n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d6e30",
   "metadata": {},
   "source": [
    "**Tokenize the dataset**\n",
    "\n",
    "We tokenize the dataset. The tokenized examples contain the column names 'attention_mask' which is a mask for padding tokens and 'input_ids' which is the id of each token corrsponding to a word. We drop the text as that is not needed anymore. Also note that we duplicate the inputs to add our labels. This is because the model of the ðŸ¤— Transformers library apply the shifting to the right, so we don't need to do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22681185",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8378a87d444142f0935c7527695f4264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=38.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'labels'],\n",
       "    num_rows: 37031\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"], max_length=512, padding='max_length', truncation=True)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "#Make dataset format pytorch tensors\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# Finally, select a subset if wanted\n",
    "train_set = tokenized_dataset#.select(list(range(10)))\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fefa121",
   "metadata": {},
   "source": [
    "### Training\n",
    "First, setup training args.\n",
    "The last argument to setup everything so we can push the model to the Hub regularly during training..\n",
    "\n",
    "Then pass training args to Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e6034ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveTokenizer(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback used to save the tokenizer whenever a model checkpoint is saved.\n",
    "    \"\"\"\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        tokenizer.save_pretrained(model_name)\n",
    "\n",
    "        \n",
    "ce_loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    The compute function needs to receive a tuple (with logits and labels)\n",
    "    and has to return a dictionary with string keys (the name of the metric) and float values.\n",
    "    It will be called at the end of each evaluation phase on the whole arrays of predictions/labels.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    # Calculate perplexity https://huggingface.co/transformers/perplexity.html\n",
    "    # \"the exponentiation of the cross-entropy between the data and model predictions.\"\n",
    "    \n",
    "    perplexity = math.exp(ce_loss(logits, labels))\n",
    "    \n",
    "    return {'perplexity': perplexity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ce3eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "batch_size = 1 # 1:34:39 for one epoch (no evaluation steps) with batch_size = 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    model_name,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    save_steps=2000,\n",
    "    save_total_limit=1,\n",
    "    log_level='info',\n",
    "    logging_steps=250\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[SaveTokenizer],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf3df559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 37031\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 18516\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18516' max='18516' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18516/18516 1:12:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.388100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.548800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.712600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.701700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>3.589300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.503300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>3.266800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.330300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>3.328800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.579200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>3.279900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>3.372600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>3.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.394000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>3.319800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.253100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>3.272500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.272300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>3.342200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.187200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>3.339100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.174300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>3.269700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.250800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>3.188400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.395700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>3.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>3.311100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.168700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>3.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>3.286900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.134100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>3.255700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>3.202200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.196800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>3.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>3.328100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>3.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.172300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>3.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.303200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>2.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.258300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>2.926800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.169100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13250</td>\n",
       "      <td>3.133700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.214600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13750</td>\n",
       "      <td>3.142600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.269300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14250</td>\n",
       "      <td>3.202100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.131700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14750</td>\n",
       "      <td>3.135800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.146500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15250</td>\n",
       "      <td>3.200500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15750</td>\n",
       "      <td>3.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.982800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16250</td>\n",
       "      <td>3.197600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16750</td>\n",
       "      <td>3.162400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17250</td>\n",
       "      <td>3.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>2.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17750</td>\n",
       "      <td>3.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.129200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18250</td>\n",
       "      <td>2.967800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.002700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to movie-plot-generation-from-scratch\\checkpoint-2000\n",
      "Configuration saved in movie-plot-generation-from-scratch\\checkpoint-2000\\config.json\n",
      "Model weights saved in movie-plot-generation-from-scratch\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in movie-plot-generation-from-scratch\\tokenizer_config.json\n",
      "Special tokens file saved in movie-plot-generation-from-scratch\\special_tokens_map.json\n",
      "Saving model checkpoint to movie-plot-generation-from-scratch\\checkpoint-4000\n",
      "Configuration saved in movie-plot-generation-from-scratch\\checkpoint-4000\\config.json\n",
      "Model weights saved in movie-plot-generation-from-scratch\\checkpoint-4000\\pytorch_model.bin\n",
      "Deleting older checkpoint [movie-plot-generation-from-scratch\\checkpoint-2000] due to args.save_total_limit\n",
      "tokenizer config file saved in movie-plot-generation-from-scratch\\tokenizer_config.json\n",
      "Special tokens file saved in movie-plot-generation-from-scratch\\special_tokens_map.json\n",
      "Saving model checkpoint to movie-plot-generation-from-scratch\\checkpoint-6000\n",
      "Configuration saved in movie-plot-generation-from-scratch\\checkpoint-6000\\config.json\n",
      "Model weights saved in movie-plot-generation-from-scratch\\checkpoint-6000\\pytorch_model.bin\n",
      "Deleting older checkpoint [movie-plot-generation-from-scratch\\checkpoint-4000] due to args.save_total_limit\n",
      "tokenizer config file saved in movie-plot-generation-from-scratch\\tokenizer_config.json\n",
      "Special tokens file saved in movie-plot-generation-from-scratch\\special_tokens_map.json\n",
      "Saving model checkpoint to movie-plot-generation-from-scratch\\checkpoint-8000\n",
      "Configuration saved in movie-plot-generation-from-scratch\\checkpoint-8000\\config.json\n",
      "Model weights saved in movie-plot-generation-from-scratch\\checkpoint-8000\\pytorch_model.bin\n",
      "Deleting older checkpoint [movie-plot-generation-from-scratch\\checkpoint-6000] due to args.save_total_limit\n",
      "tokenizer config file saved in movie-plot-generation-from-scratch\\tokenizer_config.json\n",
      "Special tokens file saved in movie-plot-generation-from-scratch\\special_tokens_map.json\n",
      "Saving model checkpoint to movie-plot-generation-from-scratch\\checkpoint-10000\n",
      "Configuration saved in movie-plot-generation-from-scratch\\checkpoint-10000\\config.json\n",
      "Model weights saved in movie-plot-generation-from-scratch\\checkpoint-10000\\pytorch_model.bin\n",
      "Deleting older checkpoint [movie-plot-generation-from-scratch\\checkpoint-8000] due to args.save_total_limit\n",
      "tokenizer config file saved in movie-plot-generation-from-scratch\\tokenizer_config.json\n",
      "Special tokens file saved in movie-plot-generation-from-scratch\\special_tokens_map.json\n",
      "Saving model checkpoint to movie-plot-generation-from-scratch\\checkpoint-12000\n",
      "Configuration saved in movie-plot-generation-from-scratch\\checkpoint-12000\\config.json\n",
      "Model weights saved in movie-plot-generation-from-scratch\\checkpoint-12000\\pytorch_model.bin\n",
      "Deleting older checkpoint [movie-plot-generation-from-scratch\\checkpoint-10000] due to args.save_total_limit\n",
      "tokenizer config file saved in movie-plot-generation-from-scratch\\tokenizer_config.json\n",
      "Special tokens file saved in movie-plot-generation-from-scratch\\special_tokens_map.json\n",
      "Saving model checkpoint to movie-plot-generation-from-scratch\\checkpoint-14000\n",
      "Configuration saved in movie-plot-generation-from-scratch\\checkpoint-14000\\config.json\n",
      "Model weights saved in movie-plot-generation-from-scratch\\checkpoint-14000\\pytorch_model.bin\n",
      "Deleting older checkpoint [movie-plot-generation-from-scratch\\checkpoint-12000] due to args.save_total_limit\n",
      "tokenizer config file saved in movie-plot-generation-from-scratch\\tokenizer_config.json\n",
      "Special tokens file saved in movie-plot-generation-from-scratch\\special_tokens_map.json\n",
      "Saving model checkpoint to movie-plot-generation-from-scratch\\checkpoint-16000\n",
      "Configuration saved in movie-plot-generation-from-scratch\\checkpoint-16000\\config.json\n",
      "Model weights saved in movie-plot-generation-from-scratch\\checkpoint-16000\\pytorch_model.bin\n",
      "Deleting older checkpoint [movie-plot-generation-from-scratch\\checkpoint-14000] due to args.save_total_limit\n",
      "tokenizer config file saved in movie-plot-generation-from-scratch\\tokenizer_config.json\n",
      "Special tokens file saved in movie-plot-generation-from-scratch\\special_tokens_map.json\n",
      "Saving model checkpoint to movie-plot-generation-from-scratch\\checkpoint-18000\n",
      "Configuration saved in movie-plot-generation-from-scratch\\checkpoint-18000\\config.json\n",
      "Model weights saved in movie-plot-generation-from-scratch\\checkpoint-18000\\pytorch_model.bin\n",
      "Deleting older checkpoint [movie-plot-generation-from-scratch\\checkpoint-16000] due to args.save_total_limit\n",
      "tokenizer config file saved in movie-plot-generation-from-scratch\\tokenizer_config.json\n",
      "Special tokens file saved in movie-plot-generation-from-scratch\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in movie-plot-generation-from-scratch\\config.json\n",
      "Model weights saved in movie-plot-generation-from-scratch\\pytorch_model.bin\n",
      "tokenizer config file saved in movie-plot-generation-from-scratch\\tokenizer_config.json\n",
      "Special tokens file saved in movie-plot-generation-from-scratch\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('movie-plot-generation-from-scratch\\\\tokenizer_config.json',\n",
       " 'movie-plot-generation-from-scratch\\\\special_tokens_map.json',\n",
       " 'movie-plot-generation-from-scratch\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results=trainer.train()\n",
    "pickle.dump(train_results, open(model_name+\"/train_results.pickle\", \"wb\")) #Load: train_results = pickle.load(open(\"train_results.pickle\", \"rb\"))\n",
    "\n",
    "model.save_pretrained(model_name)\n",
    "tokenizer.save_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89be0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Inference test\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "stories = generator(\"<BOS> <drama> Expecting the unexpected <SEP>\", max_length=512, num_return_sequences=1)\n",
    "print(*[story['generated_text'] + \"\\n\\n\\n------------------------\\n\" for story in stories])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399c21f",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned, we're ready to instantiate our Trainer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
