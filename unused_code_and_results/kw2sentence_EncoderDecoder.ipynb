{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a559a305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    TrainerCallback,\n",
    "    GPT2Config,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    AdamW,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from transformers import BartTokenizer, BartTokenizerFast, BartModel, BartForConditionalGeneration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b98c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model_name = 'BART-movie-plot-generator'\n",
    "tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-base')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.bos_token)\n",
    "print(tokenizer.eos_token)\n",
    "print(tokenizer.sep_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22165f5b",
   "metadata": {},
   "source": [
    "We need to change the dataset tokens to fit the pre-trained tokenizer tokens and add tokens for all genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d54c69",
   "metadata": {},
   "source": [
    "# Load and process dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c379c5e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load dataset from text file called \"data.txt\" and split into train/val\n",
    "dataset = load_dataset(\"text\", data_files=\"data_top_15_genres.txt\")['train']\n",
    "\n",
    "def processText(example):\n",
    "    example['text'] = [ re.sub('<BOS>', '<s>', text) for text in example['text'] ]\n",
    "    example['text'] = [ re.sub('<EOS>', '</s>', text) for text in example['text'] ]\n",
    "    example['text'] = [ re.sub('<SEP>', '</s>', text) for text in example['text'] ]    \n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(processText, batched=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047bc04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbb865",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "We now need to tokenize the dataset. The original tokenizer don't have all special tokens we require."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d5c9e",
   "metadata": {},
   "source": [
    "We need to add the special tokens that we use in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac15f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens for each genre\n",
    "genres = ['romantic drama', 'short film', 'family film',\n",
    "          'adventure', 'action/adventure', 'indie',\n",
    "          'black-and-white', 'horror', 'crime fiction',\n",
    "          'world cinema', 'action', 'thriller', \n",
    "          'romance film', 'comedy', 'drama']\n",
    "\n",
    "print(f'Number of added genres: {len(genres)}')\n",
    "special_tokens_dict = {'additional_special_tokens': [f'<{genre}>' for genre in genres]}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# We must resize token embeddings since new special tokens were added\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(model.config.vocab_size, tokenizer.vocab_size + len(tokenizer.get_added_vocab()))\n",
    "assert(model.config.vocab_size == tokenizer.vocab_size + len(tokenizer.get_added_vocab()))\n",
    "print(*tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853e6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize('<s> <drama> This is the title </s> here is the plot </s>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d6e30",
   "metadata": {},
   "source": [
    "**Tokenize the dataset**\n",
    "\n",
    "We tokenize the dataset. The tokenized examples contain the column names 'attention_mask' which is a mask for padding tokens and 'input_ids' which is the id of each token corrsponding to a word. We drop the text as that is not needed anymore. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f618dc0",
   "metadata": {},
   "source": [
    "Note that we duplicate the inputs to add our labels. This is because the model of the ðŸ¤— Transformers library apply the shifting to the right, so we don't need to do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22681185",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    padding='max_length' to pad to a length specified by the max_length argument \n",
    "    or the maximum length accepted by the model.\n",
    "    truncation=True to truncate each sequence to the maximum length accepted by the model\n",
    "    \"\"\"\n",
    "    #result = tokenizer(examples[\"text\"], padding='max_length', truncation=True) # Max input according to model(1024)\n",
    "    result = tokenizer(examples[\"text\"], max_length=512, padding='max_length', truncation=True)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "#Make dataset format pytorch tensors\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f555394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select subset if wanted\n",
    "train_set = tokenized_dataset#.select(list(range(10)))\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fefa121",
   "metadata": {},
   "source": [
    "### Training\n",
    "First, setup training args.\n",
    "The last argument to setup everything so we can push the model to the Hub regularly during training..\n",
    "\n",
    "Then pass training args to Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6034ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveTokenizer(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback used to save the tokenizer whenever a model checkpoint is saved.\n",
    "    \"\"\"\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        tokenizer.save_pretrained(finetuned_model_name)\n",
    "\n",
    "        \n",
    "ce_loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    The compute function needs to receive a tuple (with logits and labels)\n",
    "    and has to return a dictionary with string keys (the name of the metric) and float values.\n",
    "    It will be called at the end of each evaluation phase on the whole arrays of predictions/labels.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    # Calculate perplexity https://huggingface.co/transformers/perplexity.html\n",
    "    # \"the exponentiation of the cross-entropy between the data and model predictions.\"\n",
    "    \n",
    "    perplexity = math.exp(ce_loss(logits, labels))\n",
    "    \n",
    "    return {'perplexity': perplexity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce3eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "batch_size = 1 # 1:34:39 for one epoch (no evaluation steps) with batch_size = 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    finetuned_model_name,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    save_steps=2500,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[SaveTokenizer],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3df559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_results=trainer.train()\n",
    "pickle.dump(train_results, open(finetuned_model_name+\"/train_results.pickle\", \"wb\")) #Load: train_results = pickle.load(open(\"train_results.pickle\", \"rb\"))\n",
    "model.save_pretrained(finetuned_model_name)\n",
    "tokenizer.save_pretrained(finetuned_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89be0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference test\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "stories = generator(\"<s> <horror> Testing <\\s>\", max_length=1024, num_return_sequences=4)\n",
    "print(*[story['generated_text'] + \"\\n\\n\\n------------------------\\n\" for story in stories])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
